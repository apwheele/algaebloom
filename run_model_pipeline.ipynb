{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import feat, mod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "today = feat.today_str()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condensed Model Pipeline\n",
    "* In this notebook, we put all of the functions from other files used in the ADS and hyperparameter tune for all three of the final models used in the ADS\n",
    "* To improve the runtime, we sample the data and only use data after 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor, Dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Setting the global seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Just easier function to reset indices\n",
    "def split(data,test_size=0.28,random_state=10):\n",
    "    train, test = train_test_split(data,test_size=test_size,random_state=random_state)\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    return train, test\n",
    "\n",
    "def split_weight(data,test_size=0.28,weight='pred_split',random_state=10):\n",
    "    test = data.sample(test_size,weights='split_pred', random_state=random_state)\n",
    "    train = data[~data['uid'].isin(test['uid'])].reset_index(drop=True)\n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "    return train, test\n",
    "\n",
    "# Just a wrapper around sklearn\n",
    "# so it returns a pandas dataframe with named\n",
    "# columns\n",
    "class DumEnc():\n",
    "    def __init__(self,dtype=int):\n",
    "        self.OHE = OneHotEncoder(dtype=dtype,\n",
    "                                 handle_unknown='ignore',\n",
    "                                 sparse=False)\n",
    "        self.var_names = None\n",
    "    def fit(self, X):\n",
    "        self.OHE.fit(X)\n",
    "        cats = self.OHE.categories_\n",
    "        var = list(X)\n",
    "        vn = []\n",
    "        for v,ca in zip(var,cats):\n",
    "            for c in ca:\n",
    "                vn.append(f'{v}_{c}')\n",
    "        self.var_names = vn\n",
    "    def transform(self,X):\n",
    "        res = pd.DataFrame(self.OHE.transform(X),\n",
    "                           columns=self.var_names,\n",
    "                           index=X.index)\n",
    "        return res\n",
    "\n",
    "def dummy_stats(values,begin_date):\n",
    "    vdate = pd.to_datetime(values,errors='ignore')\n",
    "    year = vdate.dt.year\n",
    "    month = vdate.dt.month\n",
    "    week_day = vdate.dt.dayofweek\n",
    "    diff_days = (vdate - begin_date).dt.days\n",
    "    # if binary, turn week/month into dummy variables\n",
    "    return diff_days, week_day, month, year\n",
    "\n",
    "def circle_stats(values,begin_date):\n",
    "    vdate = pd.to_datetime(values,errors='ignore')\n",
    "    within_year = vdate.dt.dayofyear\n",
    "    week_day = vdate.dt.dayofweek\n",
    "    # calculate sine/cosine for within year\n",
    "    year_cos = np.cos(within_year*(2*np.pi/365))\n",
    "    year_sin = np.sin(within_year*(2*np.pi/365))\n",
    "    # calculate sine/cosine for within week\n",
    "    week_cos = np.cos(week_day*(2*np.pi/7))\n",
    "    week_sin = np.sin(week_day*(2*np.pi/7))\n",
    "    diff_days = (vdate - begin_date).dt.days\n",
    "    return diff_days, year_cos, year_sin, week_cos, week_sin\n",
    "\n",
    "\n",
    "class DateEnc():\n",
    "    def __init__(self,\n",
    "                 begin = '1/1/2015',\n",
    "                 dummy = True,\n",
    "                 dum_types=['days','weekday','month']):\n",
    "        self.begin = pd.to_datetime(begin)\n",
    "        self.dummy = dummy\n",
    "        # 'days','weekday','month','year'\n",
    "        self.dum_types = dum_types\n",
    "        self.cat_vars = []\n",
    "        # Setting categorical variables\n",
    "    def fit(self,X):\n",
    "        # These are just fixed functions\n",
    "        pass\n",
    "    def transform(self,X):\n",
    "        vars = list(X)\n",
    "        res = []\n",
    "        res_labs = []\n",
    "        cat_labs = []\n",
    "        if self.dummy:\n",
    "            for v in vars:\n",
    "                dd, week_day, month, year = dummy_stats(X[v],self.begin)\n",
    "                if 'days' in self.dum_types:\n",
    "                    res.append(dd) # this is not likely to be categorical\n",
    "                    res_labs.append(f'days_{v}')\n",
    "                if 'weekday' in self.dum_types:\n",
    "                    res.append(week_day)\n",
    "                    res_labs.append(f'weekday_{v}')\n",
    "                    cat_labs.append(f'weekday_{v}')\n",
    "                if 'month' in self.dum_types:\n",
    "                    res.append(month)\n",
    "                    res_labs.append(f'month_{v}')\n",
    "                    cat_labs.append(f'weekday_{v}')\n",
    "                if 'year' in self.dum_types:\n",
    "                    res.append(year)\n",
    "                    res_labs.append(f'year_{v}')\n",
    "                    cat_labs.append(f'year_{v}')\n",
    "            self.cat_vars = cat_labs\n",
    "        else:\n",
    "             for v in vars:\n",
    "                dd, year_cos, year_sin, week_cos, week_sin = circle_stats(X[v],self.begin)\n",
    "                res += [dd, year_cos, year_sin, week_cos, week_sin]\n",
    "                res_labs += [f'days_{v}',f'yearcos_{v}',f'yearsin_{v}',f'weekcos_{v}',f'weeksin_{v}']\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        res_df.index = X.index\n",
    "        res_df.columns = res_labs\n",
    "        return res_df\n",
    "\n",
    "\n",
    "# Spline encoding\n",
    "# defaults to regular knots\n",
    "# or specified locations\n",
    "class SplEnc():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X):\n",
    "        pass\n",
    "    def transform(self,X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class IdentEnc():\n",
    "    def __init__(self):\n",
    "        self.note = None\n",
    "    def fit(self,X):\n",
    "        pass\n",
    "    def transform(self,X):\n",
    "        return X.copy()\n",
    "\n",
    "\n",
    "class SimpleOrdEnc():\n",
    "    def __init__(self,\n",
    "                 dtype=int,\n",
    "                 unknown_value=-1,\n",
    "                 lim_k=None,\n",
    "                 lim_count=None):\n",
    "        self.unknown_value = unknown_value\n",
    "        self.dtype = dtype\n",
    "        self.lim_k = lim_k\n",
    "        self.lim_count = lim_count\n",
    "        self.vars = None\n",
    "        self.soe = None\n",
    "    def fit(self, X):\n",
    "        self.vars = list(X)\n",
    "        # Now creating fit for each variable\n",
    "        res_oe = {}\n",
    "        for v in list(X):\n",
    "            res_oe[v] = OrdinalEncoder(dtype=self.dtype,\n",
    "                handle_unknown='use_encoded_value',\n",
    "                        unknown_value=self.unknown_value)\n",
    "            # Get unique values minus missing\n",
    "            xc = X[v].value_counts().reset_index()\n",
    "            xc.columns = [v, \"Freq\"]\n",
    "            # If lim_k, only taking top K value\n",
    "            if self.lim_k:\n",
    "                top_k = self.lim_k - 1\n",
    "                un_vals = xc.loc[0:top_k,:]\n",
    "            # If count, using that to filter\n",
    "            elif self.lim_count:\n",
    "                un_vals = xc[xc[\"Freq\"] >= self.lim_count].copy()\n",
    "            # If neither\n",
    "            else:\n",
    "                un_vals = xc\n",
    "            # Now fitting the encoder for one variable\n",
    "            res_oe[v].fit(un_vals[[v]])\n",
    "        # Appending back to the big class\n",
    "        self.soe = res_oe\n",
    "    # Defining transform/inverse_transform classes\n",
    "    def transform(self, X):\n",
    "        xcop = X[self.vars].copy()\n",
    "        for v in self.vars:\n",
    "            xcop[v] = self.soe[v].transform( X[[v]].fillna(self.unknown_value) )\n",
    "        return xcop\n",
    "    def fit_transform(self,X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    def inverse_transform(self, X):\n",
    "        xcop = X[self.vars].copy()\n",
    "        for v in self.vars:\n",
    "            xcop[v] = self.soe[v].inverse_transform( X[[v]].fillna(self.unknown_value) )\n",
    "        return xcop\n",
    "\n",
    "# You can compose multiple feature engines together\n",
    "# Such as DateEnc() and DumEnc()\n",
    "class ComposeFE():\n",
    "    def __init__(self,\n",
    "                mods):\n",
    "        # expects a dictionary\n",
    "        # of models\n",
    "        self.mods = mods\n",
    "    def fit(self,X):\n",
    "        trans = X.copy()\n",
    "        # goes left to right\n",
    "        for k,m in self.mods.items():\n",
    "            m.fit(trans)\n",
    "            trans = m.transform(trans)\n",
    "    def transform(self,X):\n",
    "        trans = X.copy()\n",
    "        for k,m in self.mods.items():\n",
    "            trans = m.transform(trans)\n",
    "        return trans\n",
    "\n",
    "\n",
    "class FeatureEngine():\n",
    "    def __init__(self,\n",
    "                 ord_vars = None,\n",
    "                 dum_vars = None,\n",
    "                 spl_vars = None,\n",
    "                 dat_vars = None,\n",
    "                 ide_vars = None,\n",
    "                 scale = None):\n",
    "        self.fin_vars = None\n",
    "        self.enc_dict = {}\n",
    "        self.ord_vars = ord_vars\n",
    "        self.dum_vars = dum_vars\n",
    "        self.spl_vars = spl_vars\n",
    "        self.dat_vars = dat_vars\n",
    "        self.ide_vars = ide_vars\n",
    "        self.cat_vars = None\n",
    "        self.ord = SimpleOrdEnc()\n",
    "        self.dum = DumEnc()\n",
    "        self.dat = DateEnc()\n",
    "        self.spl = SplEnc()\n",
    "        self.ide = IdentEnc()\n",
    "        self.scale = scale\n",
    "        enc_vars = [ord_vars,dum_vars,spl_vars,dat_vars,ide_vars]\n",
    "        enc_mods = [self.ord, self.dum, self.spl, self.dat, self.ide]\n",
    "        for v,m in zip(enc_vars,enc_mods):\n",
    "            if v is not None:\n",
    "                self.enc_dict[tuple(v)] = m\n",
    "    def fit(self, X):\n",
    "        res = []\n",
    "        for v,m in self.enc_dict.items():\n",
    "            m.fit(X[list(v)])\n",
    "            rf = m.transform(X[list(v)])\n",
    "            res.append(rf)\n",
    "        # doing a transform to know the variable names in the end\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        if self.scale is not None:\n",
    "            self.scale.fit(res_df)\n",
    "            res_df = pd.DataFrame(self.scale.transform(res_df),columns=list(res_df))\n",
    "        self.fin_vars = list(res_df)\n",
    "        # Adding categorical variables back in\n",
    "        cat_vars = []\n",
    "        if self.dum_vars is not None:\n",
    "            cat_vars += self.dum.var_names\n",
    "        if self.dat_vars is not None:\n",
    "            cat_vars += self.dat.cat_vars\n",
    "        if self.ord_vars is not None:\n",
    "            cat_vars += self.ord_vars\n",
    "        self.cat_vars = cat_vars\n",
    "        return res_df\n",
    "    def transform(self, X):\n",
    "        res = []\n",
    "        for v,m in self.enc_dict.items():\n",
    "            res.append(m.transform(X[list(v)]))\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        if self.scale is not None:\n",
    "            res_df = pd.DataFrame(self.scale.transform(res_df),columns=list(res_df))\n",
    "        return res_df\n",
    "\n",
    "def safelog(x):\n",
    "    return np.log(x.clip(1))\n",
    "\n",
    "def strat(values):\n",
    "    edges = [np.NINF,20000,1e6,1e7,1e8,np.inf]\n",
    "    labs = [1,2,3,4,5]\n",
    "    res = pd.cut(values,bins=edges,labels=labs,right=False)\n",
    "    return res.astype(int)\n",
    "\n",
    "\n",
    "# Class to insert different types of\n",
    "# regression models\n",
    "class RegMod():\n",
    "    def __init__(self,\n",
    "                 ord_vars = None,\n",
    "                 dum_vars = None,\n",
    "                 dat_vars = None,\n",
    "                 ide_vars = None,\n",
    "                        y = None,\n",
    "                transform = None,\n",
    "                inv_trans = None,\n",
    "                   weight = None,\n",
    "                  scale_x = None,\n",
    "                      mod = XGBRegressor(n_estimators=100, max_depth=3)):\n",
    "        self.fe = FeatureEngine(ord_vars=ord_vars,\n",
    "                           dat_vars=dat_vars,\n",
    "                           dum_vars=dum_vars,\n",
    "                           ide_vars=ide_vars,\n",
    "                           scale = scale_x)\n",
    "        self.transform = transform\n",
    "        self.inv_trans = inv_trans\n",
    "        self.mod = mod\n",
    "        self.y = y\n",
    "        self.resids = None\n",
    "        self.cat_vars = None\n",
    "        self.weight = weight\n",
    "        self.metrics = None\n",
    "        self.fit_cat = False\n",
    "    def fit(self, X, weight=True, cat=True):\n",
    "        if (self.weight is not None) & weight:\n",
    "            sw = X[self.weight]\n",
    "        else:\n",
    "            sw = None\n",
    "            #print('NOT using Weights in fit')\n",
    "        y_dat = X[self.y].copy()\n",
    "        if self.transform:\n",
    "            y_dat = self.transform(y_dat)\n",
    "        X_dat = self.fe.fit(X)\n",
    "        self.cat_vars = self.fe.cat_vars\n",
    "        # If catboost or lightgbm, pass in categories\n",
    "        if (type(self.mod) == CatBoostRegressor) & cat:\n",
    "            vt = list(X_dat)\n",
    "            if self.cat_vars is not None:\n",
    "                ci = [vt.index(c) for c in self.cat_vars]\n",
    "            else:\n",
    "                ci = None\n",
    "            self.mod.fit(X_dat,y_dat,sample_weight=sw,cat_features=ci)\n",
    "            self.fit_cat = True\n",
    "        elif (type(self.mod) == LGBMRegressor) & cat:\n",
    "            self.fit_cat = True\n",
    "            for v in self.cat_vars:\n",
    "                X_dat[v] = X_dat[v].astype('category')\n",
    "            self.mod.fit(X_dat, y_dat, sample_weight=sw)\n",
    "        else:\n",
    "            self.mod.fit(X_dat,y_dat,sample_weight=sw)\n",
    "        pred = self.mod.predict(X_dat)\n",
    "        self.resids = pd.Series(y_dat - pred)\n",
    "    def predict(self,X,duan=True):\n",
    "        X_dat = self.fe.transform(X)\n",
    "        if self.fit_cat & (type(self.mod) == LGBMRegressor):\n",
    "            for v in self.cat_vars:\n",
    "                X_dat[v] = X_dat[v].astype('category')\n",
    "        pred = pd.Series(self.mod.predict(X_dat), X.index)\n",
    "        resids = self.resids\n",
    "        # if transform, do Duans smearing\n",
    "        if (self.transform is not None) & duan:\n",
    "            resids = resids.values.reshape(1,resids.shape[0])\n",
    "            dp = self.inv_trans(pred.values.reshape(X.shape[0],1) + resids)\n",
    "            pred = pd.Series(dp.mean(axis=1), X.index)\n",
    "        return pred\n",
    "    def predict_int(self,X):\n",
    "        pred = self.predict(X)\n",
    "        if self.transform:\n",
    "            pred = strat(pred)\n",
    "        pred = pred.clip(1,5).round().astype(int)\n",
    "        return pred\n",
    "    def feat_import(self):\n",
    "        var_li = self.fe.fin_vars\n",
    "        mod_fi = self.mod.feature_importances_\n",
    "        res_df = pd.DataFrame(zip(var_li,mod_fi),columns = ['Var','FI'])\n",
    "        res_df.sort_values('FI',ascending=False,inplace=True,ignore_index=True)\n",
    "        # Normalize to sum to 1\n",
    "        res_df['FI'] = res_df['FI']/res_df['FI'].sum()\n",
    "        return res_df\n",
    "    def met_eval(self, data, weight=True, cat=True, full_train=False,\n",
    "                 split_tt='weighted', test_size=500, test_splits=10, \n",
    "                 ret=False, pr=False):\n",
    "        dc = data.copy()\n",
    "        seeds = np.random.randint(1,1e6,test_splits)\n",
    "        metrics = []\n",
    "        for s in seeds:\n",
    "            if split_tt == 'weighted':\n",
    "                if self.weight is not None:\n",
    "                    wv = self.weight\n",
    "                else:\n",
    "                    wv = 'pred_split'\n",
    "                train, test = split_weight(dc,test_size,wv,s)\n",
    "            else:\n",
    "                train, test = split(dc,test_size,s)\n",
    "            self.fit(train, weight, cat)\n",
    "            test['pred'] = self.predict_int(test)\n",
    "            met_di = rmse_region(test,ret=True)\n",
    "            met_di['seed'] = s\n",
    "            metrics.append(met_di.copy())\n",
    "        mpd = pd.DataFrame(metrics)\n",
    "        if full_train:\n",
    "            self.fit(data)\n",
    "        if self.metrics is None:\n",
    "            self.metrics = mpd\n",
    "        else:\n",
    "            self.metrics = pd.concat([self.metrics,mpd],axis=0)\n",
    "        if pr:\n",
    "            print(mpd[['AvgError','midwest','northeast','south','west']].describe().T)\n",
    "        if ret:\n",
    "            return mpd['AvgError'].mean()\n",
    "\n",
    "class CatMod():\n",
    "    def __init__(self,\n",
    "                 ord_vars = None,\n",
    "                 dum_vars = None,\n",
    "                 dat_vars = None,\n",
    "                 ide_vars = None,\n",
    "                        y = None,\n",
    "                transform = None,\n",
    "                inv_trans = None,\n",
    "                  scale_x = None,\n",
    "                      mod = CatBoostClassifier(iterations=100,depth=5,allow_writing_files=False,verbose=False)\n",
    "                      ):\n",
    "        self.fe = FeatureEngine(ord_vars=ord_vars,\n",
    "                           dat_vars=dat_vars,\n",
    "                           dum_vars=dum_vars,\n",
    "                           ide_vars=ide_vars,\n",
    "                           scale = scale_x)\n",
    "        self.mod = mod\n",
    "        self.y = y\n",
    "        self.cat_vars = None\n",
    "    def fit(self, X, sample_weight=None):\n",
    "        y_dat = X[self.y].copy().astype(int)\n",
    "        X_dat = self.fe.fit(X)\n",
    "        self.mod.fit(X_dat,y_dat,sample_weight=sample_weight)\n",
    "    def predict_proba(self,X):\n",
    "        X_dat = self.fe.transform(X)\n",
    "        pred_probs = self.mod.predict_proba(X_dat)\n",
    "        # Turning into nicer dataframe\n",
    "        cols = [f'P{str(i)}' for i in range(pred_probs.shape[1])]\n",
    "        pred_probs = pd.DataFrame(pred_probs,index=X.index, columns=cols)\n",
    "        return pred_probs\n",
    "    def predict(self,X):\n",
    "        # returns predicted probability\n",
    "        pred = self.predict_proba(X)\n",
    "        return pred[\"P1\"]\n",
    "    def feat_import(self):\n",
    "        var_li = self.fe.fin_vars\n",
    "        mod_fi = self.mod.feature_importances_\n",
    "        res_df = pd.DataFrame(zip(var_li,mod_fi),columns = ['Var','FI'])\n",
    "        res_df.sort_values('FI',ascending=False,inplace=True,ignore_index=True)\n",
    "        # Normalize to sum to 1\n",
    "        res_df['FI'] = res_df['FI']/res_df['FI'].sum()\n",
    "        return res_df\n",
    "\n",
    "# If you pass in multiple models\n",
    "# this will ensemble them, presumes regressor models\n",
    "class EnsMod():\n",
    "    def __init__(self, mods, av_func = 'mean'):\n",
    "        self.mods = mods #should be dict\n",
    "        self.av_func = av_func\n",
    "    def fit(self,X,weight=True,cat=True):\n",
    "        for key,mod in self.mods.items():\n",
    "            mod.fit(X,weight=weight,cat=cat)\n",
    "    def predict(self,X):\n",
    "        res = []\n",
    "        for key,mod in self.mods.items():\n",
    "            res.append(mod.predict(X))\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        if self.av_func == 'mean':\n",
    "            pred = res_df.mean(axis=1)\n",
    "        return pred\n",
    "    def predict_int(self,X):\n",
    "        pred = self.predict(X)\n",
    "        pred = pred.clip(1,5).round().astype(int)\n",
    "        return pred\n",
    "\n",
    "def rmse_region(data,pred='pred',true='severity',region='region',scale=False,ret=False):\n",
    "    dc = data[[region,true,pred]].copy()\n",
    "    if scale:\n",
    "        dc[pred] = strat(dc[pred])\n",
    "    dc[pred] = dc[pred].round().astype(int).clip(1,5)\n",
    "    dc[region] = dc[region].replace({1:'northeast',\n",
    "                                     2:'south',\n",
    "                                     3:'midwest',\n",
    "                                     4:'west'})\n",
    "    dc['sq_error'] = (dc[true] - dc[pred])**2\n",
    "    gr_val = dc.groupby(region,as_index=False)['sq_error'].mean()\n",
    "    gr_val['root_mse'] = np.sqrt(gr_val['sq_error'])\n",
    "    avg_error = gr_val['root_mse'].mean()\n",
    "    if ret:\n",
    "        regions = gr_val['region'].tolist()\n",
    "        regions.append('AvgError')\n",
    "        vals = gr_val['root_mse'].tolist()\n",
    "        vals.append(avg_error)\n",
    "        gr_di = {r:v for r,v in zip(regions,vals)}\n",
    "        return gr_di\n",
    "    else:\n",
    "        print(f'\\nAverage error {avg_error:.4f}')\n",
    "        print('\\nRegion Error')\n",
    "        print(gr_val[['region','root_mse']])\n",
    "\n",
    "def save_model(mod,name):\n",
    "    fname = f'./models/{name}.pkl'\n",
    "    outfile = open(fname,\"wb\")\n",
    "    pickle.dump(mod,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "def load_model(name):\n",
    "    fname = f'./models/{name}.pkl'\n",
    "    infile = open(fname, \"rb\")\n",
    "    mod = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return mod\n",
    "\n",
    "\n",
    "# function to check if similar to any past submissions\n",
    "def check_similar(current):\n",
    "    files = os.listdir(\"./submissions\")\n",
    "    for fi in files:\n",
    "        old = pd.read_csv(f\"./submissions/{fi}\")\n",
    "        dif = np.abs(current['severity'] - old['severity']).sum()\n",
    "        if dif == 0:\n",
    "            print(f'Date {fi} same as current')\n",
    "\n",
    "\n",
    "def check_day(current,day=\"sub_2023_01_31.csv\"):\n",
    "    old = pd.read_csv(fr\"./submissions/{day}\")\n",
    "    dstr = f'dif_{day[4:-4]}'\n",
    "    current[dstr] = old['severity'] - current['severity']\n",
    "    print(current[dstr].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def today_str():\n",
    "    now = datetime.now()\n",
    "    return now.strftime('%Y_%m_%d')\n",
    "\n",
    "\n",
    "reg_ord = {'west':4,\n",
    "           'midwest':3,\n",
    "           'south':2,\n",
    "           'northeast':1}\n",
    "\n",
    "# ordinal encoding for region\n",
    "def org_reg(data,rstr='region'):\n",
    "    data[rstr] = data[rstr].replace(reg_ord)\n",
    "\n",
    "def filter_reg(data, region):\n",
    "    return data.loc[data['region']==region]\n",
    "\n",
    "def subsample(data, n):\n",
    "    nrows = data.shape[0]\n",
    "    stratified_sample, _ = train_test_split(data, test_size=1 - (n/nrows), stratify=data['region'])\n",
    "    return stratified_sample\n",
    "\n",
    "def filter_time(data):\n",
    "    return data.loc[data['date'].dt.year > 2016]\n",
    "\n",
    "def ord_imtype(data,imstr='imtype'):\n",
    "    rep_di = {'land_sat':0,\n",
    "              'sentinel':1}\n",
    "    data[imstr] = data[imstr].fillna(-1).replace(rep_di)\n",
    "\n",
    "\n",
    "def filter_landsat(data):\n",
    "    rep_di = {'land_sat':0,\n",
    "              'sentinel':1}\n",
    "    data['imtype'] = data['imtype'].fillna(-1).replace(rep_di)\n",
    "    im_vars = ['prop_lake_500', 'r_500', 'g_500', 'b_500']\n",
    "    im_vars += ['prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']\n",
    "    im_vars += ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500']\n",
    "    im_vars += ['imtype']\n",
    "    landsat = data['imtype'] == 0\n",
    "    data.loc[landsat,im_vars] = -1\n",
    "\n",
    "def safesqrt(values):\n",
    "    return np.sqrt(values.clip(0))\n",
    "\n",
    "def safelog(x):\n",
    "    return np.log(x.clip(1))\n",
    "\n",
    "def strat(values):\n",
    "    edges = [np.NINF,20000,1e6,1e7,1e8,np.inf]\n",
    "    labs = [1,2,3,4,5]\n",
    "    res = pd.cut(values,bins=edges,labels=labs,right=False)\n",
    "    return res.astype(int)\n",
    "\n",
    "# Looking at train/test\n",
    "# there are a few clusters, want\n",
    "# to make sure to predict these well\n",
    "def cluster(x):\n",
    "    lat, lon = x[0], x[1]\n",
    "    if (lat < 41) & (lon < -116):\n",
    "        # cali\n",
    "        return 7\n",
    "    elif (lat < 41) & (lat > 36.29) & (lon < -92.9) & (lon > -102.2):\n",
    "        # midwest\n",
    "        return 6\n",
    "    elif (lat < 38.14) & (lat > 33.26) & (lon < -74.8) & (lon > -85.52):\n",
    "        # carolina\n",
    "        return 2\n",
    "    elif (lat < 43) & (lat > 38.7) & (lon < -75.4) & (lon > -83.55):\n",
    "        # erie\n",
    "        return 3\n",
    "    elif (lat < 43.1) & (lat > 40.7) & (lon < -69.5) & (lon > -74.6):\n",
    "        # mass\n",
    "        return 4\n",
    "    elif (lat < 49.6) & (lat > 41.5) & (lon < -83.55) & (lon > -104.56):\n",
    "        # dakota\n",
    "        return 1\n",
    "    else:\n",
    "        # other\n",
    "        return 5\n",
    "\n",
    "#                  1   2      2   3    3     4   4    5   5\n",
    "#te_st = pd.Series([1,20000,30000,1e6,1e6+1,1e7,1e7+1,1e8,1e9])\n",
    "#print(strat(te_st))\n",
    "\n",
    "db = './data/data.sqlite'\n",
    "\n",
    "train_query = \"\"\"\n",
    "SELECT \n",
    "  m.uid,\n",
    "  l.region,\n",
    "  l.severity,\n",
    "  l.density,\n",
    "  m.latitude,\n",
    "  m.longitude,\n",
    "  m.date,\n",
    "  e.elevation,\n",
    "  e.mine,\n",
    "  e.maxe,\n",
    "  e.dife,\n",
    "  e.avge,\n",
    "  e.stde,\n",
    "  sl.severity_100,\n",
    "  sl.logDensity_100,\n",
    "  sl.count_100,\n",
    "  sl.severity_300,\n",
    "  sl.logDensity_300,\n",
    "  sl.count_300,\n",
    "  sl.severity_1000,\n",
    "  sl.logDensity_1000,\n",
    "  sl.count_1000,\n",
    "  st.imtype,\n",
    "  st.prop_lake_500,\n",
    "  st.r_500,\n",
    "  st.g_500,\n",
    "  st.b_500,\n",
    "  st.prop_lake_1000,\n",
    "  st.r_1000,\n",
    "  st.g_1000,\n",
    "  st.b_1000,\n",
    "  st.prop_lake_2500,\n",
    "  st.r_2500,\n",
    "  st.g_2500,\n",
    "  st.b_2500\n",
    "FROM meta AS m\n",
    "LEFT JOIN elevation_dem AS e\n",
    "  ON m.uid = e.uid\n",
    "LEFT JOIN spat_lag AS sl\n",
    "  ON m.uid = sl.uid\n",
    "LEFT JOIN sat AS st\n",
    "  ON m.uid = st.uid\n",
    "LEFT JOIN labels AS l\n",
    "  ON m.uid = l.uid\n",
    "WHERE\n",
    "  m.split = 'train'\n",
    "\"\"\"\n",
    "\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "  m.uid,\n",
    "  l.region,\n",
    "  m.latitude,\n",
    "  m.longitude,\n",
    "  m.date,\n",
    "  e.elevation,\n",
    "  e.mine,\n",
    "  e.maxe,\n",
    "  e.dife,\n",
    "  e.avge,\n",
    "  e.stde,\n",
    "  sl.severity_100,\n",
    "  sl.logDensity_100,\n",
    "  sl.count_100,\n",
    "  sl.severity_300,\n",
    "  sl.logDensity_300,\n",
    "  sl.count_300,\n",
    "  sl.severity_1000,\n",
    "  sl.logDensity_1000,\n",
    "  sl.count_1000,\n",
    "  st.imtype,\n",
    "  st.prop_lake_500,\n",
    "  st.r_500,\n",
    "  st.g_500,\n",
    "  st.b_500,\n",
    "  st.prop_lake_1000,\n",
    "  st.r_1000,\n",
    "  st.g_1000,\n",
    "  st.b_1000,\n",
    "  st.prop_lake_2500,\n",
    "  st.r_2500,\n",
    "  st.g_2500,\n",
    "  st.b_2500\n",
    "FROM meta AS m\n",
    "LEFT JOIN elevation_dem AS e\n",
    "  ON m.uid = e.uid\n",
    "LEFT JOIN spat_lag AS sl\n",
    "  ON m.uid = sl.uid\n",
    "LEFT JOIN sat AS st\n",
    "  ON m.uid = st.uid\n",
    "LEFT JOIN format AS l\n",
    "  ON m.uid = l.uid\n",
    "WHERE\n",
    "  m.split = 'test'\n",
    "\"\"\"\n",
    "\n",
    "def add_table(data,tab_name,db_str=db):\n",
    "    db_con = sqlite3.connect(db_str)\n",
    "    dn = data.copy()\n",
    "    dn['DateTime'] = pd.to_datetime('now',utc=True)\n",
    "    dn.to_sql(tab_name,index=False,if_exists='replace',con=db_con)\n",
    "\n",
    "\n",
    "def get_both(db_str=db,split_pred=False):\n",
    "    r1 = get_data('train',db_str,split_pred)\n",
    "    r1['test'] = 0\n",
    "    r1.drop(columns=['severity','density','logDensity'],inplace=True)\n",
    "    r2 = get_data('test',db_str,split_pred)\n",
    "    r2['test'] = 1\n",
    "    res_df = pd.concat([r1,r2],axis=0)\n",
    "    return res_df.reset_index(drop=True)\n",
    "\n",
    "def get_data(region, data_type='train',db_str=db,split_pred=False):\n",
    "    db_con = sqlite3.connect(db_str)\n",
    "    if data_type == 'train':\n",
    "        sql = train_query\n",
    "    elif data_type == 'test':\n",
    "        sql = test_query\n",
    "    dat = pd.read_sql(sql,con=db_con)\n",
    "    org_reg(dat) # Region ordinal encode\n",
    "    # Winning solution used landsat-7 data\n",
    "    #ord_imtype(dat) # image type landsat/sentinel\n",
    "    filter_landsat(dat) # filtering mistake landsat-7 info\n",
    "    dat = dat.fillna(-1) # missing a bit of sat data\n",
    "    dat['cluster'] = dat[['latitude','longitude']].apply(cluster,axis=1)\n",
    "    dat['date'] = pd.to_datetime(dat['date'])\n",
    "    #dat = filter_time(dat)\n",
    "    dat = subsample(dat, 3000)\n",
    "    if data_type == 'train':\n",
    "        dat['logDensity'] = safelog(dat['density'])\n",
    "    if split_pred:\n",
    "        pred_test = pd.read_sql('SELECT uid, pred AS split_pred FROM split_pred',con=db_con)\n",
    "        dat = dat.merge(pred_test,on='uid')\n",
    "    return dat\n",
    "\n",
    "\n",
    "# Need logic to take predictions and get them in the right order\n",
    "def sub_format(data,pred='pred'):\n",
    "    form = pd.read_csv('./data/submission_format.csv')\n",
    "    # some logic to transform predictions via Duan\n",
    "    # smearing\n",
    "    dp = data[[pred,'uid']].copy()\n",
    "    dp[pred] = dp[pred].round().astype(int).clip(1,5)\n",
    "    mf = form.merge(dp,on='uid')\n",
    "    mf['severity'] = mf['pred']\n",
    "    return mf[['uid','region','severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows: 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>elevation</th>\n",
       "      <th>mine</th>\n",
       "      <th>maxe</th>\n",
       "      <th>...</th>\n",
       "      <th>r_1000</th>\n",
       "      <th>g_1000</th>\n",
       "      <th>b_1000</th>\n",
       "      <th>prop_lake_2500</th>\n",
       "      <th>r_2500</th>\n",
       "      <th>g_2500</th>\n",
       "      <th>b_2500</th>\n",
       "      <th>cluster</th>\n",
       "      <th>logDensity</th>\n",
       "      <th>split_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gckx</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.753000e+03</td>\n",
       "      <td>35.720480</td>\n",
       "      <td>-79.077754</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>115.129219</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>163.073227</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7.469084</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mhcv</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114235e+03</td>\n",
       "      <td>42.056766</td>\n",
       "      <td>-78.879216</td>\n",
       "      <td>2016-06-08</td>\n",
       "      <td>403.500000</td>\n",
       "      <td>403.500000</td>\n",
       "      <td>578.112427</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.015924</td>\n",
       "      <td>0.023589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pxet</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.203000e+03</td>\n",
       "      <td>35.976000</td>\n",
       "      <td>-78.734517</td>\n",
       "      <td>2014-11-12</td>\n",
       "      <td>126.463593</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>140.864914</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7.697575</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hkuc</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>41.331450</td>\n",
       "      <td>-75.046390</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>413.677216</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>482.507904</td>\n",
       "      <td>...</td>\n",
       "      <td>225.027731</td>\n",
       "      <td>231.797394</td>\n",
       "      <td>225.153053</td>\n",
       "      <td>0.077932</td>\n",
       "      <td>207.285977</td>\n",
       "      <td>215.550381</td>\n",
       "      <td>206.428199</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paeg</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.780500e+06</td>\n",
       "      <td>37.322200</td>\n",
       "      <td>-122.084000</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>152.873901</td>\n",
       "      <td>115.062286</td>\n",
       "      <td>297.888214</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>15.380056</td>\n",
       "      <td>0.089089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid  region  severity       density   latitude   longitude       date  \\\n",
       "0  gckx       2         1  1.753000e+03  35.720480  -79.077754 2015-09-17   \n",
       "1  mhcv       1         1  1.114235e+03  42.056766  -78.879216 2016-06-08   \n",
       "2  pxet       2         1  2.203000e+03  35.976000  -78.734517 2014-11-12   \n",
       "3  hkuc       1         1  0.000000e+00  41.331450  -75.046390 2020-08-26   \n",
       "4  paeg       4         4  4.780500e+06  37.322200 -122.084000 2020-02-17   \n",
       "\n",
       "    elevation        mine        maxe  ...      r_1000      g_1000  \\\n",
       "0  115.129219   64.000000  163.073227  ...   -1.000000   -1.000000   \n",
       "1  403.500000  403.500000  578.112427  ...   -1.000000   -1.000000   \n",
       "2  126.463593   74.000000  140.864914  ...   -1.000000   -1.000000   \n",
       "3  413.677216  390.000000  482.507904  ...  225.027731  231.797394   \n",
       "4  152.873901  115.062286  297.888214  ...   -1.000000   -1.000000   \n",
       "\n",
       "       b_1000  prop_lake_2500      r_2500      g_2500      b_2500  cluster  \\\n",
       "0   -1.000000       -1.000000   -1.000000   -1.000000   -1.000000        2   \n",
       "1   -1.000000       -1.000000   -1.000000   -1.000000   -1.000000        3   \n",
       "2   -1.000000       -1.000000   -1.000000   -1.000000   -1.000000        2   \n",
       "3  225.153053        0.077932  207.285977  215.550381  206.428199        5   \n",
       "4   -1.000000       -1.000000   -1.000000   -1.000000   -1.000000        7   \n",
       "\n",
       "   logDensity  split_pred  \n",
       "0    7.469084    0.001412  \n",
       "1    7.015924    0.023589  \n",
       "2    7.697575    0.001595  \n",
       "3    0.000000    0.025384  \n",
       "4   15.380056    0.089089  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dat = get_data(1, split_pred=True)\n",
    "print(\"num rows:\", train_dat.shape[0])\n",
    "train_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example just predicting severity directly\n",
    "sat_500 = ['prop_lake_500', 'r_500', 'g_500', 'b_500']\n",
    "sat_1000 = ['prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']\n",
    "sat_2500 = ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500']\n",
    "sat_1025 = ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500', \n",
    "           'prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "res_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fit with weights\n",
    "weight_cats = tuple([False])\n",
    "\n",
    "# Model fit with categorical variables\n",
    "cat_cats = (True,False)\n",
    "\n",
    "# Lat/Lon included in model\n",
    "xy_cats = {'no': [],\n",
    "           'both': ['latitude','longitude'],\n",
    "           'lat': ['latitude'],\n",
    "           'lon': ['longitude']}\n",
    "\n",
    "xy_keys = tuple(xy_cats.keys())\n",
    "\n",
    "# Region variables\n",
    "region_cats = {'both': ['region','cluster'],\n",
    "               'reg':  ['region'],\n",
    "               'clust':  ['cluster']}\n",
    "\n",
    "reg_keys = tuple(region_cats.keys())\n",
    "\n",
    "# Elevation Variables\n",
    "ele_cats = {'max_dif':['maxe','dife'],\n",
    "            'all_var':['maxe','dife','elevation','stde'],\n",
    "            'ele_std':['elevation','stde'],\n",
    "            'ele_dif':['elevation','dife'],\n",
    "            'max_std':['maxe','stde'] }\n",
    "\n",
    "ele_keys = tuple(ele_cats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Lag Variables\n",
    "sl_cats = {#'lag100': ['severity_100','logDensity_100','count_100'],\n",
    "           #'lag300': ['severity_300','logDensity_300','count_300'],\n",
    "           #'lag1000': ['severity_1000','logDensity_1000','count_1000'],\n",
    "           'lagNone': []}\n",
    "\n",
    "sl_keys = tuple(sl_cats.keys())\n",
    "\n",
    "# Sat imagery data\n",
    "sat_cats = {'sat500': ['imtype','prop_lake_500','r_500','g_500','b_500'],\n",
    "            'sat1000': ['imtype','prop_lake_1000','r_1000','g_1000','b_1000'],\n",
    "            'sat2500': ['imtype','prop_lake_2500','r_2500','g_2500','b_2500'],\n",
    "            'sat500_1000': ['imtype','prop_lake_500','r_500','g_500','b_500', 'prop_lake_1000','r_1000','g_1000','b_1000'],\n",
    "            'sat500_2500': ['imtype','prop_lake_500','r_500','g_500','b_500', 'prop_lake_2500','r_2500','g_2500','b_2500'],\n",
    "            'sat1000_2500': ['imtype','prop_lake_1000','r_1000','g_1000','b_1000', 'prop_lake_2500','r_2500','g_2500','b_2500']}\n",
    "\n",
    "sat_keys = tuple(sat_cats.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 19:25:24,008]\u001b[0m A new study created in memory with name: no-name-9bf1401f-ab6c-4498-be0b-4592022e03c3\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:25:24,023]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 50, 'max_depth': 10, 'ele_vars': 'all_var', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000_2500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\4223045919.py\", line 28, in objective_lgb\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True,weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:25:24,024]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_rmse\n\u001b[0;32m     31\u001b[0m study_lgb \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m study_lgb\u001b[39m.\u001b[39;49moptimize(objective_lgb, n_trials\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m) \u001b[39m# 150\u001b[39;00m\n\u001b[0;32m     33\u001b[0m trial_lgb \u001b[39m=\u001b[39m study_lgb\u001b[39m.\u001b[39mbest_trial\n\u001b[0;32m     34\u001b[0m res_results[\u001b[39m'\u001b[39m\u001b[39mlgb\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trial_lgb\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[55], line 28\u001b[0m, in \u001b[0;36mobjective_lgb\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     19\u001b[0m cv \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sat_cats[param[\u001b[39m'\u001b[39m\u001b[39msat_set\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     20\u001b[0m rm \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mRegMod(ord_vars\u001b[39m=\u001b[39mov,\n\u001b[0;32m     21\u001b[0m                 dum_vars\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m                 dat_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 mod \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mLGBMRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39mround\u001b[39m(param[\u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m     27\u001b[0m                                         max_depth\u001b[39m=\u001b[39mparam[\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m---> 28\u001b[0m avg_rmse \u001b[39m=\u001b[39m rm\u001b[39m.\u001b[39;49mmet_eval(train_dat,ret\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,weight\u001b[39m=\u001b[39;49mparam[\u001b[39m'\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m'\u001b[39;49m],cat\u001b[39m=\u001b[39;49mparam[\u001b[39m'\u001b[39;49m\u001b[39mcat_type\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m avg_rmse\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:391\u001b[0m, in \u001b[0;36mRegMod.met_eval\u001b[1;34m(self, data, weight, cat, full_train, split_tt, test_size, test_splits, ret, pr)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     train, test \u001b[39m=\u001b[39m split(dc,test_size,s)\n\u001b[1;32m--> 391\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(train, weight, cat)\n\u001b[0;32m    392\u001b[0m test[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_int(test)\n\u001b[0;32m    393\u001b[0m met_di \u001b[39m=\u001b[39m rmse_region(test,ret\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:329\u001b[0m, in \u001b[0;36mRegMod.fit\u001b[1;34m(self, X, weight, cat)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m    328\u001b[0m     y_dat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(y_dat)\n\u001b[1;32m--> 329\u001b[0m X_dat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfe\u001b[39m.\u001b[39;49mfit(X)\n\u001b[0;32m    330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfe\u001b[39m.\u001b[39mcat_vars\n\u001b[0;32m    331\u001b[0m \u001b[39m# If catboost or lightgbm, pass in categories\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:254\u001b[0m, in \u001b[0;36mFeatureEngine.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    252\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[0;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m v,m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 254\u001b[0m     m\u001b[39m.\u001b[39;49mfit(X[\u001b[39mlist\u001b[39;49m(v)])\n\u001b[0;32m    255\u001b[0m     rf \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mtransform(X[\u001b[39mlist\u001b[39m(v)])\n\u001b[0;32m    256\u001b[0m     res\u001b[39m.\u001b[39mappend(rf)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:185\u001b[0m, in \u001b[0;36mSimpleOrdEnc.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    183\u001b[0m         un_vals \u001b[39m=\u001b[39m xc\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Now fitting the encoder for one variable\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     res_oe[v]\u001b[39m.\u001b[39;49mfit(un_vals[[v]])\n\u001b[0;32m    186\u001b[0m \u001b[39m# Appending back to the big class\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoe \u001b[39m=\u001b[39m res_oe\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1258\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munknown_value should only be set when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandle_unknown is \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1257\u001b[0m \u001b[39m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[1;32m-> 1258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1261\u001b[0m     \u001b[39mfor\u001b[39;00m feature_cats \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:74\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[0;32m     75\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:62\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_features):\n\u001b[0;32m     61\u001b[0m     Xi \u001b[39m=\u001b[39m _safe_indexing(X, indices\u001b[39m=\u001b[39mi, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m     Xi \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m     63\u001b[0m         Xi, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mneeds_validation\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     65\u001b[0m     X_columns\u001b[39m.\u001b[39mappend(Xi)\n\u001b[0;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m X_columns, n_samples, n_features\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "def objective_lgb(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 600, 10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"ele_set\": trial.suggest_categorical(\"ele_vars\", ele_keys),\n",
    "        \"xy_set\": trial.suggest_categorical(\"xy_set\", xy_keys),\n",
    "        \"sl_set\": trial.suggest_categorical(\"sl_set\", sl_keys),\n",
    "        \"reg_set\": trial.suggest_categorical(\"reg_set\", reg_keys),\n",
    "        \"weight\": trial.suggest_categorical(\"weight\", weight_cats),\n",
    "        \"cat_type\": trial.suggest_categorical(\"cat_type\", cat_cats),\n",
    "        \"sat_set\": trial.suggest_categorical(\"sat_set\", sat_keys),\n",
    "    }\n",
    "    # Setting the different variables\n",
    "    ov = region_cats[param['reg_set']]\n",
    "    #if 'imtype' in sat_cats[param['sat_set']]:\n",
    "    #    ov.append('imtype')\n",
    "    cv = ele_cats[param['ele_set']] + xy_cats[param['xy_set']]\n",
    "    cv += sl_cats[param['sl_set']]\n",
    "    cv += sat_cats[param['sat_set']]\n",
    "    rm = mod.RegMod(ord_vars=ov,\n",
    "                    dum_vars=None,\n",
    "                    dat_vars=['date'],\n",
    "                    ide_vars=cv,\n",
    "                    weight = 'split_pred',\n",
    "                    y='severity',\n",
    "                    mod = mod.LGBMRegressor(n_estimators=round(param['n_estimators']),\n",
    "                                            max_depth=param['max_depth']))\n",
    "    avg_rmse = rm.met_eval(train_dat,ret=True,weight=param['weight'],cat=param['cat_type'])\n",
    "    return avg_rmse\n",
    "\n",
    "study_lgb = optuna.create_study(direction=\"minimize\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=300) # 150\n",
    "trial_lgb = study_lgb.best_trial\n",
    "res_results['lgb'] = trial_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 19:04:40,672]\u001b[0m A new study created in memory with name: no-name-dfee08d9-a149-40e6-aa31-6ec0745853fe\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:40,689]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 390, 'max_depth': 4, 'ele_vars': 'ele_dif', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'sat_set': 'sat2500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\433524742.py\", line 28, in objective_xgb\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True,weight=param['weight'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:40,691]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_rmse\n\u001b[0;32m     31\u001b[0m study_xgb \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m study_xgb\u001b[39m.\u001b[39;49moptimize(objective_xgb, n_trials\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m trial_xgb \u001b[39m=\u001b[39m study_xgb\u001b[39m.\u001b[39mbest_trial\n\u001b[0;32m     34\u001b[0m res_results[\u001b[39m'\u001b[39m\u001b[39mxgb\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trial_xgb\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m, in \u001b[0;36mobjective_xgb\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m cv \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sat_cats[param[\u001b[39m'\u001b[39m\u001b[39msat_set\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     19\u001b[0m rm \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mRegMod(ord_vars\u001b[39m=\u001b[39mov,\n\u001b[0;32m     20\u001b[0m                 dum_vars\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m                 dat_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m                                        max_depth\u001b[39m=\u001b[39mparam[\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m             )\n\u001b[1;32m---> 28\u001b[0m avg_rmse \u001b[39m=\u001b[39m rm\u001b[39m.\u001b[39;49mmet_eval(train_dat,ret\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,weight\u001b[39m=\u001b[39;49mparam[\u001b[39m'\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m avg_rmse\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:391\u001b[0m, in \u001b[0;36mRegMod.met_eval\u001b[1;34m(self, data, weight, cat, full_train, split_tt, test_size, test_splits, ret, pr)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     train, test \u001b[39m=\u001b[39m split(dc,test_size,s)\n\u001b[1;32m--> 391\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(train, weight, cat)\n\u001b[0;32m    392\u001b[0m test[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_int(test)\n\u001b[0;32m    393\u001b[0m met_di \u001b[39m=\u001b[39m rmse_region(test,ret\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:329\u001b[0m, in \u001b[0;36mRegMod.fit\u001b[1;34m(self, X, weight, cat)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m    328\u001b[0m     y_dat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(y_dat)\n\u001b[1;32m--> 329\u001b[0m X_dat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfe\u001b[39m.\u001b[39;49mfit(X)\n\u001b[0;32m    330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfe\u001b[39m.\u001b[39mcat_vars\n\u001b[0;32m    331\u001b[0m \u001b[39m# If catboost or lightgbm, pass in categories\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:254\u001b[0m, in \u001b[0;36mFeatureEngine.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    252\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[0;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m v,m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 254\u001b[0m     m\u001b[39m.\u001b[39;49mfit(X[\u001b[39mlist\u001b[39;49m(v)])\n\u001b[0;32m    255\u001b[0m     rf \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mtransform(X[\u001b[39mlist\u001b[39m(v)])\n\u001b[0;32m    256\u001b[0m     res\u001b[39m.\u001b[39mappend(rf)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:185\u001b[0m, in \u001b[0;36mSimpleOrdEnc.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    183\u001b[0m         un_vals \u001b[39m=\u001b[39m xc\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Now fitting the encoder for one variable\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     res_oe[v]\u001b[39m.\u001b[39;49mfit(un_vals[[v]])\n\u001b[0;32m    186\u001b[0m \u001b[39m# Appending back to the big class\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoe \u001b[39m=\u001b[39m res_oe\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1258\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munknown_value should only be set when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandle_unknown is \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1257\u001b[0m \u001b[39m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[1;32m-> 1258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1261\u001b[0m     \u001b[39mfor\u001b[39;00m feature_cats \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:74\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[0;32m     75\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:62\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_features):\n\u001b[0;32m     61\u001b[0m     Xi \u001b[39m=\u001b[39m _safe_indexing(X, indices\u001b[39m=\u001b[39mi, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m     Xi \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m     63\u001b[0m         Xi, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mneeds_validation\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     65\u001b[0m     X_columns\u001b[39m.\u001b[39mappend(Xi)\n\u001b[0;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m X_columns, n_samples, n_features\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 600, 10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"ele_set\": trial.suggest_categorical(\"ele_vars\", ele_keys),\n",
    "        \"xy_set\": trial.suggest_categorical(\"xy_set\", xy_keys),\n",
    "        \"sl_set\": trial.suggest_categorical(\"sl_set\", sl_keys),\n",
    "        \"reg_set\": trial.suggest_categorical(\"reg_set\", reg_keys),\n",
    "        \"weight\": trial.suggest_categorical(\"weight\", weight_cats),\n",
    "        \"sat_set\": trial.suggest_categorical(\"sat_set\", sat_keys),\n",
    "    }\n",
    "    # Setting the different variables\n",
    "    ov = region_cats[param['reg_set']]\n",
    "    #if 'imtype' in sat_cats[param['sat_set']]:\n",
    "    #    ov.append('imtype')\n",
    "    cv = ele_cats[param['ele_set']] + xy_cats[param['xy_set']]\n",
    "    cv += sl_cats[param['sl_set']]\n",
    "    cv += sat_cats[param['sat_set']]\n",
    "    rm = mod.RegMod(ord_vars=ov,\n",
    "                    dum_vars=None,\n",
    "                    dat_vars=['date'],\n",
    "                    ide_vars=cv,\n",
    "                    weight = 'split_pred',\n",
    "                    y='severity',\n",
    "                    mod = mod.XGBRegressor(n_estimators=round(param['n_estimators']),\n",
    "                                           max_depth=param['max_depth'])\n",
    "                )\n",
    "    avg_rmse = rm.met_eval(train_dat,ret=True,weight=param['weight'])\n",
    "    return avg_rmse\n",
    "\n",
    "study_xgb = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=300)\n",
    "trial_xgb = study_xgb.best_trial\n",
    "res_results['xgb'] = trial_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 19:04:42,270]\u001b[0m A new study created in memory with name: no-name-bef2a3a0-1f89-4721-b040-6d397d8400d8\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,331]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 590, 'max_depth': 4, 'ele_vars': 'ele_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': True, 'sat_set': 'sat500_2500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,336]\u001b[0m Trial 0 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,353]\u001b[0m Trial 2 failed with parameters: {'n_estimators': 90, 'max_depth': 7, 'ele_vars': 'max_dif', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': True, 'sat_set': 'sat500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,361]\u001b[0m Trial 1 failed with parameters: {'n_estimators': 460, 'max_depth': 6, 'ele_vars': 'max_dif', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,367]\u001b[0m Trial 2 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,376]\u001b[0m Trial 3 failed with parameters: {'n_estimators': 190, 'max_depth': 6, 'ele_vars': 'ele_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,383]\u001b[0m Trial 1 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,399]\u001b[0m Trial 3 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,406]\u001b[0m Trial 7 failed with parameters: {'n_estimators': 420, 'max_depth': 8, 'ele_vars': 'max_std', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': True, 'sat_set': 'sat500_1000'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,417]\u001b[0m Trial 6 failed with parameters: {'n_estimators': 80, 'max_depth': 8, 'ele_vars': 'ele_dif', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,422]\u001b[0m Trial 5 failed with parameters: {'n_estimators': 580, 'max_depth': 10, 'ele_vars': 'ele_std', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_2500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 38)\n",
      "(2000, 38)\n",
      "(2000, 38)\n",
      "(2000, 38)\n",
      "(2000, 38)\n",
      "(2000, 38)\n",
      "(2000, 38)\n",
      "(2000, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-17 19:04:42,432]\u001b[0m Trial 4 failed with parameters: {'n_estimators': 450, 'max_depth': 6, 'ele_vars': 'ele_dif', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'} because of the following error: ValueError('Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Jennah\\AppData\\Local\\Temp\\ipykernel_34788\\1555681335.py\", line 32, in objective_cat\n",
      "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 391, in met_eval\n",
      "    self.fit(train, weight, cat)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 329, in fit\n",
      "    X_dat = self.fe.fit(X)\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 254, in fit\n",
      "    m.fit(X[list(v)])\n",
      "  File \"c:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py\", line 185, in fit\n",
      "    res_oe[v].fit(un_vals[[v]])\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 1258, in fit\n",
      "    self._fit(X, handle_unknown=self.handle_unknown, force_all_finite=\"allow-nan\")\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 74, in _fit\n",
      "    X_list, n_samples, n_features = self._check_X(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\", line 62, in _check_X\n",
      "    Xi = check_array(\n",
      "  File \"c:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 931, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.\n",
      "\u001b[33m[W 2023-04-17 19:04:42,439]\u001b[0m Trial 7 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,445]\u001b[0m Trial 6 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,450]\u001b[0m Trial 5 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-17 19:04:42,478]\u001b[0m Trial 4 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m avg_rmse\n\u001b[0;32m     35\u001b[0m study_cat \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m study_cat\u001b[39m.\u001b[39;49moptimize(objective_cat, n_trials\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     37\u001b[0m trial_cat \u001b[39m=\u001b[39m study_cat\u001b[39m.\u001b[39mbest_trial\n\u001b[0;32m     38\u001b[0m res_results[\u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trial_cat\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:103\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[39m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m                     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m completed:\n\u001b[1;32m--> 103\u001b[0m                         f\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    105\u001b[0m                 futures\u001b[39m.\u001b[39madd(\n\u001b[0;32m    106\u001b[0m                     executor\u001b[39m.\u001b[39msubmit(\n\u001b[0;32m    107\u001b[0m                         _optimize_sequential,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m                     )\n\u001b[0;32m    119\u001b[0m                 )\n\u001b[0;32m    120\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[19], line 32\u001b[0m, in \u001b[0;36mobjective_cat\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     20\u001b[0m rm \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39mRegMod(ord_vars\u001b[39m=\u001b[39mov,\n\u001b[0;32m     21\u001b[0m                 dum_vars\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m                 dat_vars\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m                                             verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m             )\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(train_dat\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 32\u001b[0m avg_rmse \u001b[39m=\u001b[39m rm\u001b[39m.\u001b[39;49mmet_eval(train_dat,ret\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, weight\u001b[39m=\u001b[39;49mparam[\u001b[39m'\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m'\u001b[39;49m],cat\u001b[39m=\u001b[39;49mparam[\u001b[39m'\u001b[39;49m\u001b[39mcat_type\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m avg_rmse\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:391\u001b[0m, in \u001b[0;36mRegMod.met_eval\u001b[1;34m(self, data, weight, cat, full_train, split_tt, test_size, test_splits, ret, pr)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m     train, test \u001b[39m=\u001b[39m split(dc,test_size,s)\n\u001b[1;32m--> 391\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(train, weight, cat)\n\u001b[0;32m    392\u001b[0m test[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_int(test)\n\u001b[0;32m    393\u001b[0m met_di \u001b[39m=\u001b[39m rmse_region(test,ret\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:329\u001b[0m, in \u001b[0;36mRegMod.fit\u001b[1;34m(self, X, weight, cat)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m    328\u001b[0m     y_dat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(y_dat)\n\u001b[1;32m--> 329\u001b[0m X_dat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfe\u001b[39m.\u001b[39;49mfit(X)\n\u001b[0;32m    330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfe\u001b[39m.\u001b[39mcat_vars\n\u001b[0;32m    331\u001b[0m \u001b[39m# If catboost or lightgbm, pass in categories\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:254\u001b[0m, in \u001b[0;36mFeatureEngine.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    252\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[0;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m v,m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 254\u001b[0m     m\u001b[39m.\u001b[39;49mfit(X[\u001b[39mlist\u001b[39;49m(v)])\n\u001b[0;32m    255\u001b[0m     rf \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mtransform(X[\u001b[39mlist\u001b[39m(v)])\n\u001b[0;32m    256\u001b[0m     res\u001b[39m.\u001b[39mappend(rf)\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\Desktop\\Code\\rds_project\\algaebloom\\src\\mod.py:185\u001b[0m, in \u001b[0;36mSimpleOrdEnc.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    183\u001b[0m         un_vals \u001b[39m=\u001b[39m xc\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Now fitting the encoder for one variable\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     res_oe[v]\u001b[39m.\u001b[39;49mfit(un_vals[[v]])\n\u001b[0;32m    186\u001b[0m \u001b[39m# Appending back to the big class\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoe \u001b[39m=\u001b[39m res_oe\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1258\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munknown_value should only be set when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandle_unknown is \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1257\u001b[0m \u001b[39m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[1;32m-> 1258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1261\u001b[0m     \u001b[39mfor\u001b[39;00m feature_cats \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:74\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[0;32m     75\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:62\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_features):\n\u001b[0;32m     61\u001b[0m     Xi \u001b[39m=\u001b[39m _safe_indexing(X, indices\u001b[39m=\u001b[39mi, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m     Xi \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m     63\u001b[0m         Xi, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mneeds_validation\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     65\u001b[0m     X_columns\u001b[39m.\u001b[39mappend(Xi)\n\u001b[0;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m X_columns, n_samples, n_features\n",
      "File \u001b[1;32mc:\\Users\\Jennah\\anaconda3\\envs\\bloom\\lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "def objective_cat(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 600, 10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"ele_set\": trial.suggest_categorical(\"ele_vars\", ele_keys),\n",
    "        \"xy_set\": trial.suggest_categorical(\"xy_set\", xy_keys),\n",
    "        \"sl_set\": trial.suggest_categorical(\"sl_set\", sl_keys),\n",
    "        \"reg_set\": trial.suggest_categorical(\"reg_set\", reg_keys),\n",
    "        \"weight\": trial.suggest_categorical(\"weight\", weight_cats),\n",
    "        \"cat_type\": trial.suggest_categorical(\"cat_type\", cat_cats),\n",
    "        \"sat_set\": trial.suggest_categorical(\"sat_set\", sat_keys),\n",
    "    }\n",
    "    # Setting the different variables\n",
    "    ov = region_cats[param['reg_set']]\n",
    "    #if 'imtype' in sat_cats[param['sat_set']]:\n",
    "    #    ov.append('imtype')\n",
    "    cv = ele_cats[param['ele_set']] + xy_cats[param['xy_set']]\n",
    "    cv += sl_cats[param['sl_set']]\n",
    "    cv += sat_cats[param['sat_set']]\n",
    "    rm = mod.RegMod(ord_vars=ov,\n",
    "                    dum_vars=None,\n",
    "                    dat_vars=['date'],\n",
    "                    ide_vars=cv,\n",
    "                    weight = 'split_pred',\n",
    "                    y='severity',\n",
    "                    mod = mod.CatBoostRegressor(iterations=round(param['n_estimators']),\n",
    "                                                depth=param['max_depth'],\n",
    "                                                allow_writing_files=False,\n",
    "                                                verbose=False)\n",
    "                )\n",
    "    print(train_dat.shape)\n",
    "    avg_rmse = rm.met_eval(train_dat,ret=True, weight=param['weight'],cat=param['cat_type'])\n",
    "    return avg_rmse\n",
    "\n",
    "study_cat = optuna.create_study(direction=\"minimize\")\n",
    "study_cat.optimize(objective_cat, n_trials=300, n_jobs=-1)\n",
    "trial_cat = study_cat.best_trial\n",
    "res_results['cat'] = trial_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average RMSE LightBoost 0.8202049012609962\n",
      "Best Params\n",
      "{'n_estimators': 250, 'max_depth': 6, 'ele_vars': 'ele_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat2500'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Average RMSE LightBoost {trial_lgb.value}\")\n",
    "# Best Average RMSE LightBoost 0.8202049012609962\n",
    "print(\"Best Params\")\n",
    "print(trial_lgb.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average RMSE XGBoost 0.8301078787963169\n",
      "Best Params\n",
      "{'n_estimators': 60, 'max_depth': 7, 'ele_vars': 'ele_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'sat_set': 'sat2500'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 220,\n",
       " 'max_depth': 9,\n",
       " 'ele_vars': 'ele_std',\n",
       " 'xy_set': 'both',\n",
       " 'reg_set': 'both',\n",
       " 'weight': True}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Best Average RMSE XGBoost {trial_xgb.value}\")\n",
    "# Best Average RMSE XGBoost 0.7703229517195069\n",
    "print(\"Best Params\")\n",
    "print(trial_xgb.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Average RMSE CatBoost {trial_cat.value}\")\n",
    "#Best Average RMSE CatBoost 0.7526218612441193\n",
    "print(\"Best Params\")\n",
    "print(trial_cat.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8429004962271186"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig = mod.RegMod(ord_vars=region_cats['reg'],\n",
    "                dat_vars=['date'],\n",
    "                ide_vars=ele_cats['ele_std'] + xy_cats['both'] + sl_cats['lagNone'] + sat_2500,\n",
    "                y='severity',\n",
    "                mod = mod.LGBMRegressor(n_estimators=trial_lgb.params['n_estimators'], \n",
    "                                        max_depth=trial_lgb.params['max_depth'])\n",
    "                )\n",
    "lig.fit(train_dat,weight=trial_lgb.params['weight'],cat=trial_lgb.params['cat_type'])\n",
    "lig.met_eval(train_dat,ret=True,weight=trial_lgb.params['weight'],cat=trial_lgb.params['cat_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693361650789024"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = mod.RegMod(ord_vars=region_cats['reg'],\n",
    "                 dat_vars=['date'],\n",
    "                 ide_vars=ele_cats['ele_std'] + xy_cats['both'] + sl_cats['lagNone'] + sat_2500,\n",
    "                 y='severity',\n",
    "                 mod = mod.XGBRegressor(n_estimators=60, max_depth=9))\n",
    "xgb.fit(train_dat,weight=False)\n",
    "xgb.met_eval(train_dat,ret=True,weight=trial_xgb.params['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = mod.EnsMod(mods={'xgb': xgb, 'cat': cat, 'lig': lig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now getting files for out of sample data\n",
    "test = feat.get_data(data_type='test')\n",
    "test['pred'] = rm.predict_int(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    2512\n",
      "4    2091\n",
      "1    1241\n",
      "3     666\n",
      "Name: severity, dtype: int64\n",
      "Date sub_2023_02_16.csv same as current\n",
      "0    6510\n",
      "Name: dif_2023_02_16, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "form_dat = feat.sub_format(test)\n",
    "print(form_dat['severity'].value_counts())\n",
    "\n",
    "# function to check if similar to any past submissions\n",
    "mod.check_similar(form_dat)\n",
    "\n",
    "# Checking to see differences compared to best submission so far\n",
    "current = form_dat.copy()\n",
    "mod.check_day(current,day=\"sub_2023_02_16.csv\")\n",
    "current.groupby('region',as_index=False)['dif_2023_02_16'].value_counts()\n",
    "\n",
    "# Saving the data and model\n",
    "form_dat.to_csv(f'sub_BESTRESULTS_0216.csv',index=False)\n",
    "mod.save_model(rm,f'mod_BESTRESULTS_0216')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
