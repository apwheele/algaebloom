{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import feat, mod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "today = feat.today_str()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRC - mod.py \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor, Dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Setting the global seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# Just easier function to reset indices\n",
    "def split(data,test_size=1200,random_state=10):\n",
    "    train, test = train_test_split(data,test_size=test_size,random_state=random_state)\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    return train, test\n",
    "\n",
    "def split_weight(data,test_size=1200,weight='pred_split',random_state=10):\n",
    "    test = data.sample(test_size,weights='split_pred', random_state=random_state)\n",
    "    train = data[~data['uid'].isin(test['uid'])].reset_index(drop=True)\n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "    return train, test\n",
    "\n",
    "# Just a wrapper around sklearn\n",
    "# so it returns a pandas dataframe with named\n",
    "# columns\n",
    "class DumEnc():\n",
    "    def __init__(self,dtype=int):\n",
    "        self.OHE = OneHotEncoder(dtype=dtype,\n",
    "                                 handle_unknown='ignore',\n",
    "                                 sparse=False)\n",
    "        self.var_names = None\n",
    "    def fit(self, X):\n",
    "        self.OHE.fit(X)\n",
    "        cats = self.OHE.categories_\n",
    "        var = list(X)\n",
    "        vn = []\n",
    "        for v,ca in zip(var,cats):\n",
    "            for c in ca:\n",
    "                vn.append(f'{v}_{c}')\n",
    "        self.var_names = vn\n",
    "    def transform(self,X):\n",
    "        res = pd.DataFrame(self.OHE.transform(X),\n",
    "                           columns=self.var_names,\n",
    "                           index=X.index)\n",
    "        return res\n",
    "\n",
    "def dummy_stats(values,begin_date):\n",
    "    vdate = pd.to_datetime(values,errors='ignore')\n",
    "    year = vdate.dt.year\n",
    "    month = vdate.dt.month\n",
    "    week_day = vdate.dt.dayofweek\n",
    "    diff_days = (vdate - begin_date).dt.days\n",
    "    # if binary, turn week/month into dummy variables\n",
    "    return diff_days, week_day, month, year\n",
    "\n",
    "def circle_stats(values,begin_date):\n",
    "    vdate = pd.to_datetime(values,errors='ignore')\n",
    "    within_year = vdate.dt.dayofyear\n",
    "    week_day = vdate.dt.dayofweek\n",
    "    # calculate sine/cosine for within year\n",
    "    year_cos = np.cos(within_year*(2*np.pi/365))\n",
    "    year_sin = np.sin(within_year*(2*np.pi/365))\n",
    "    # calculate sine/cosine for within week\n",
    "    week_cos = np.cos(week_day*(2*np.pi/7))\n",
    "    week_sin = np.sin(week_day*(2*np.pi/7))\n",
    "    diff_days = (vdate - begin_date).dt.days\n",
    "    return diff_days, year_cos, year_sin, week_cos, week_sin\n",
    "\n",
    "\n",
    "class DateEnc():\n",
    "    def __init__(self,\n",
    "                 begin = '1/1/2015',\n",
    "                 dummy = True,\n",
    "                 dum_types=['days','weekday','month']):\n",
    "        self.begin = pd.to_datetime(begin)\n",
    "        self.dummy = dummy\n",
    "        # 'days','weekday','month','year'\n",
    "        self.dum_types = dum_types\n",
    "        self.cat_vars = []\n",
    "        # Setting categorical variables\n",
    "    def fit(self,X):\n",
    "        # These are just fixed functions\n",
    "        pass\n",
    "    def transform(self,X):\n",
    "        vars = list(X)\n",
    "        res = []\n",
    "        res_labs = []\n",
    "        cat_labs = []\n",
    "        if self.dummy:\n",
    "            for v in vars:\n",
    "                dd, week_day, month, year = dummy_stats(X[v],self.begin)\n",
    "                if 'days' in self.dum_types:\n",
    "                    res.append(dd) # this is not likely to be categorical\n",
    "                    res_labs.append(f'days_{v}')\n",
    "                if 'weekday' in self.dum_types:\n",
    "                    res.append(week_day)\n",
    "                    res_labs.append(f'weekday_{v}')\n",
    "                    cat_labs.append(f'weekday_{v}')\n",
    "                if 'month' in self.dum_types:\n",
    "                    res.append(month)\n",
    "                    res_labs.append(f'month_{v}')\n",
    "                    cat_labs.append(f'weekday_{v}')\n",
    "                if 'year' in self.dum_types:\n",
    "                    res.append(year)\n",
    "                    res_labs.append(f'year_{v}')\n",
    "                    cat_labs.append(f'year_{v}')\n",
    "            self.cat_vars = cat_labs\n",
    "        else:\n",
    "             for v in vars:\n",
    "                dd, year_cos, year_sin, week_cos, week_sin = circle_stats(X[v],self.begin)\n",
    "                res += [dd, year_cos, year_sin, week_cos, week_sin]\n",
    "                res_labs += [f'days_{v}',f'yearcos_{v}',f'yearsin_{v}',f'weekcos_{v}',f'weeksin_{v}']\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        res_df.index = X.index\n",
    "        res_df.columns = res_labs\n",
    "        return res_df\n",
    "\n",
    "\n",
    "# Spline encoding\n",
    "# defaults to regular knots\n",
    "# or specified locations\n",
    "class SplEnc():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X):\n",
    "        pass\n",
    "    def transform(self,X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class IdentEnc():\n",
    "    def __init__(self):\n",
    "        self.note = None\n",
    "    def fit(self,X):\n",
    "        pass\n",
    "    def transform(self,X):\n",
    "        return X.copy()\n",
    "\n",
    "\n",
    "class SimpleOrdEnc():\n",
    "    def __init__(self,\n",
    "                 dtype=int,\n",
    "                 unknown_value=-1,\n",
    "                 lim_k=None,\n",
    "                 lim_count=None):\n",
    "        self.unknown_value = unknown_value\n",
    "        self.dtype = dtype\n",
    "        self.lim_k = lim_k\n",
    "        self.lim_count = lim_count\n",
    "        self.vars = None\n",
    "        self.soe = None\n",
    "    def fit(self, X):\n",
    "        self.vars = list(X)\n",
    "        # Now creating fit for each variable\n",
    "        res_oe = {}\n",
    "        for v in list(X):\n",
    "            res_oe[v] = OrdinalEncoder(dtype=self.dtype,\n",
    "                handle_unknown='use_encoded_value',\n",
    "                        unknown_value=self.unknown_value)\n",
    "            # Get unique values minus missing\n",
    "            xc = X[v].value_counts().reset_index()\n",
    "            xc.columns = [v, \"Freq\"]\n",
    "            # If lim_k, only taking top K value\n",
    "            if self.lim_k:\n",
    "                top_k = self.lim_k - 1\n",
    "                un_vals = xc.loc[0:top_k,:]\n",
    "            # If count, using that to filter\n",
    "            elif self.lim_count:\n",
    "                un_vals = xc[xc[\"Freq\"] >= self.lim_count].copy()\n",
    "            # If neither\n",
    "            else:\n",
    "                un_vals = xc\n",
    "            # Now fitting the encoder for one variable\n",
    "            res_oe[v].fit(un_vals[[v]])\n",
    "        # Appending back to the big class\n",
    "        self.soe = res_oe\n",
    "    # Defining transform/inverse_transform classes\n",
    "    def transform(self, X):\n",
    "        xcop = X[self.vars].copy()\n",
    "        for v in self.vars:\n",
    "            xcop[v] = self.soe[v].transform( X[[v]].fillna(self.unknown_value) )\n",
    "        return xcop\n",
    "    def fit_transform(self,X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    def inverse_transform(self, X):\n",
    "        xcop = X[self.vars].copy()\n",
    "        for v in self.vars:\n",
    "            xcop[v] = self.soe[v].inverse_transform( X[[v]].fillna(self.unknown_value) )\n",
    "        return xcop\n",
    "\n",
    "# You can compose multiple feature engines together\n",
    "# Such as DateEnc() and DumEnc()\n",
    "class ComposeFE():\n",
    "    def __init__(self,\n",
    "                mods):\n",
    "        # expects a dictionary\n",
    "        # of models\n",
    "        self.mods = mods\n",
    "    def fit(self,X):\n",
    "        trans = X.copy()\n",
    "        # goes left to right\n",
    "        for k,m in self.mods.items():\n",
    "            m.fit(trans)\n",
    "            trans = m.transform(trans)\n",
    "    def transform(self,X):\n",
    "        trans = X.copy()\n",
    "        for k,m in self.mods.items():\n",
    "            trans = m.transform(trans)\n",
    "        return trans\n",
    "\n",
    "\n",
    "class FeatureEngine():\n",
    "    def __init__(self,\n",
    "                 ord_vars = None,\n",
    "                 dum_vars = None,\n",
    "                 spl_vars = None,\n",
    "                 dat_vars = None,\n",
    "                 ide_vars = None,\n",
    "                 scale = None):\n",
    "        self.fin_vars = None\n",
    "        self.enc_dict = {}\n",
    "        self.ord_vars = ord_vars\n",
    "        self.dum_vars = dum_vars\n",
    "        self.spl_vars = spl_vars\n",
    "        self.dat_vars = dat_vars\n",
    "        self.ide_vars = ide_vars\n",
    "        self.cat_vars = None\n",
    "        self.ord = SimpleOrdEnc()\n",
    "        self.dum = DumEnc()\n",
    "        self.dat = DateEnc()\n",
    "        self.spl = SplEnc()\n",
    "        self.ide = IdentEnc()\n",
    "        self.scale = scale\n",
    "        enc_vars = [ord_vars,dum_vars,spl_vars,dat_vars,ide_vars]\n",
    "        enc_mods = [self.ord, self.dum, self.spl, self.dat, self.ide]\n",
    "        for v,m in zip(enc_vars,enc_mods):\n",
    "            if v is not None:\n",
    "                self.enc_dict[tuple(v)] = m\n",
    "    def fit(self, X):\n",
    "        res = []\n",
    "        for v,m in self.enc_dict.items():\n",
    "            m.fit(X[list(v)])\n",
    "            rf = m.transform(X[list(v)])\n",
    "            res.append(rf)\n",
    "        # doing a transform to know the variable names in the end\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        if self.scale is not None:\n",
    "            self.scale.fit(res_df)\n",
    "            res_df = pd.DataFrame(self.scale.transform(res_df),columns=list(res_df))\n",
    "        self.fin_vars = list(res_df)\n",
    "        # Adding categorical variables back in\n",
    "        cat_vars = []\n",
    "        if self.dum_vars is not None:\n",
    "            cat_vars += self.dum.var_names\n",
    "        if self.dat_vars is not None:\n",
    "            cat_vars += self.dat.cat_vars\n",
    "        if self.ord_vars is not None:\n",
    "            cat_vars += self.ord_vars\n",
    "        self.cat_vars = cat_vars\n",
    "        return res_df\n",
    "    def transform(self, X):\n",
    "        res = []\n",
    "        for v,m in self.enc_dict.items():\n",
    "            res.append(m.transform(X[list(v)]))\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        if self.scale is not None:\n",
    "            res_df = pd.DataFrame(self.scale.transform(res_df),columns=list(res_df))\n",
    "        return res_df\n",
    "\n",
    "def safelog(x):\n",
    "    return np.log(x.clip(1))\n",
    "\n",
    "def strat(values):\n",
    "    edges = [np.NINF,20000,1e6,1e7,1e8,np.inf]\n",
    "    labs = [1,2,3,4,5]\n",
    "    res = pd.cut(values,bins=edges,labels=labs,right=False)\n",
    "    return res.astype(int)\n",
    "\n",
    "\n",
    "# Class to insert different types of\n",
    "# regression models\n",
    "class RegMod():\n",
    "    def __init__(self,\n",
    "                 ord_vars = None,\n",
    "                 dum_vars = None,\n",
    "                 dat_vars = None,\n",
    "                 ide_vars = None,\n",
    "                        y = None,\n",
    "                transform = None,\n",
    "                inv_trans = None,\n",
    "                   weight = None,\n",
    "                  scale_x = None,\n",
    "                      mod = XGBRegressor(n_estimators=100, max_depth=3)):\n",
    "        self.fe = FeatureEngine(ord_vars=ord_vars,\n",
    "                           dat_vars=dat_vars,\n",
    "                           dum_vars=dum_vars,\n",
    "                           ide_vars=ide_vars,\n",
    "                           scale = scale_x)\n",
    "        self.transform = transform\n",
    "        self.inv_trans = inv_trans\n",
    "        self.mod = mod\n",
    "        self.y = y\n",
    "        self.resids = None\n",
    "        self.cat_vars = None\n",
    "        self.weight = weight\n",
    "        self.metrics = None\n",
    "        self.fit_cat = False\n",
    "    def fit(self, X, weight=True, cat=True):\n",
    "        if (self.weight is not None) & weight:\n",
    "            sw = X[self.weight]\n",
    "        else:\n",
    "            sw = None\n",
    "            #print('NOT using Weights in fit')\n",
    "        y_dat = X[self.y].copy()\n",
    "        if self.transform:\n",
    "            y_dat = self.transform(y_dat)\n",
    "        X_dat = self.fe.fit(X)\n",
    "        self.cat_vars = self.fe.cat_vars\n",
    "        # If catboost or lightgbm, pass in categories\n",
    "        if (type(self.mod) == CatBoostRegressor) & cat:\n",
    "            vt = list(X_dat)\n",
    "            if self.cat_vars is not None:\n",
    "                ci = [vt.index(c) for c in self.cat_vars]\n",
    "            else:\n",
    "                ci = None\n",
    "            self.mod.fit(X_dat,y_dat,sample_weight=sw,cat_features=ci)\n",
    "            self.fit_cat = True\n",
    "        elif (type(self.mod) == LGBMRegressor) & cat:\n",
    "            self.fit_cat = True\n",
    "            for v in self.cat_vars:\n",
    "                X_dat[v] = X_dat[v].astype('category')\n",
    "            self.mod.fit(X_dat, y_dat, sample_weight=sw)\n",
    "        else:\n",
    "            self.mod.fit(X_dat,y_dat,sample_weight=sw)\n",
    "        pred = self.mod.predict(X_dat)\n",
    "        self.resids = pd.Series(y_dat - pred)\n",
    "    def predict(self,X,duan=True):\n",
    "        X_dat = self.fe.transform(X)\n",
    "        if self.fit_cat & (type(self.mod) == LGBMRegressor):\n",
    "            for v in self.cat_vars:\n",
    "                X_dat[v] = X_dat[v].astype('category')\n",
    "        pred = pd.Series(self.mod.predict(X_dat), X.index)\n",
    "        resids = self.resids\n",
    "        # if transform, do Duans smearing\n",
    "        if (self.transform is not None) & duan:\n",
    "            resids = resids.values.reshape(1,resids.shape[0])\n",
    "            dp = self.inv_trans(pred.values.reshape(X.shape[0],1) + resids)\n",
    "            pred = pd.Series(dp.mean(axis=1), X.index)\n",
    "        return pred\n",
    "    def predict_int(self,X):\n",
    "        pred = self.predict(X)\n",
    "        if self.transform:\n",
    "            pred = strat(pred)\n",
    "        pred = pred.clip(1,5).round().astype(int)\n",
    "        return pred\n",
    "    def feat_import(self):\n",
    "        var_li = self.fe.fin_vars\n",
    "        mod_fi = self.mod.feature_importances_\n",
    "        res_df = pd.DataFrame(zip(var_li,mod_fi),columns = ['Var','FI'])\n",
    "        res_df.sort_values('FI',ascending=False,inplace=True,ignore_index=True)\n",
    "        # Normalize to sum to 1\n",
    "        res_df['FI'] = res_df['FI']/res_df['FI'].sum()\n",
    "        return res_df\n",
    "    def met_eval(self, data, weight=True, cat=True, full_train=False,\n",
    "                 split_tt='weighted', test_size=2000, test_splits=10, \n",
    "                 ret=False, pr=False):\n",
    "        dc = data.copy()\n",
    "        seeds = np.random.randint(1,1e6,test_splits)\n",
    "        metrics = []\n",
    "        for s in seeds:\n",
    "            if split_tt == 'weighted':\n",
    "                if self.weight is not None:\n",
    "                    wv = self.weight\n",
    "                else:\n",
    "                    wv = 'pred_split'\n",
    "                train, test = split_weight(dc,test_size,wv,s)\n",
    "            else:\n",
    "                train, test = split(dc,test_size,s)\n",
    "            self.fit(train, weight, cat)\n",
    "            test['pred'] = self.predict_int(test)\n",
    "            met_di = rmse_region(test,ret=True)\n",
    "            met_di['seed'] = s\n",
    "            metrics.append(met_di.copy())\n",
    "        mpd = pd.DataFrame(metrics)\n",
    "        if full_train:\n",
    "            self.fit(data)\n",
    "        if self.metrics is None:\n",
    "            self.metrics = mpd\n",
    "        else:\n",
    "            self.metrics = pd.concat([self.metrics,mpd],axis=0)\n",
    "        if pr:\n",
    "            print(mpd[['AvgError','midwest','northeast','south','west']].describe().T)\n",
    "        if ret:\n",
    "            return mpd['AvgError'].mean()\n",
    "\n",
    "class CatMod():\n",
    "    def __init__(self,\n",
    "                 ord_vars = None,\n",
    "                 dum_vars = None,\n",
    "                 dat_vars = None,\n",
    "                 ide_vars = None,\n",
    "                        y = None,\n",
    "                transform = None,\n",
    "                inv_trans = None,\n",
    "                  scale_x = None,\n",
    "                      mod = CatBoostClassifier(iterations=100,depth=5,allow_writing_files=False,verbose=False)\n",
    "                      ):\n",
    "        self.fe = FeatureEngine(ord_vars=ord_vars,\n",
    "                           dat_vars=dat_vars,\n",
    "                           dum_vars=dum_vars,\n",
    "                           ide_vars=ide_vars,\n",
    "                           scale = scale_x)\n",
    "        self.mod = mod\n",
    "        self.y = y\n",
    "        self.cat_vars = None\n",
    "    def fit(self, X, sample_weight=None):\n",
    "        y_dat = X[self.y].copy().astype(int)\n",
    "        X_dat = self.fe.fit(X)\n",
    "        self.mod.fit(X_dat,y_dat,sample_weight=sample_weight)\n",
    "    def predict_proba(self,X):\n",
    "        X_dat = self.fe.transform(X)\n",
    "        pred_probs = self.mod.predict_proba(X_dat)\n",
    "        # Turning into nicer dataframe\n",
    "        cols = [f'P{str(i)}' for i in range(pred_probs.shape[1])]\n",
    "        pred_probs = pd.DataFrame(pred_probs,index=X.index, columns=cols)\n",
    "        return pred_probs\n",
    "    def predict(self,X):\n",
    "        # returns predicted probability\n",
    "        pred = self.predict_proba(X)\n",
    "        return pred[\"P1\"]\n",
    "    def feat_import(self):\n",
    "        var_li = self.fe.fin_vars\n",
    "        mod_fi = self.mod.feature_importances_\n",
    "        res_df = pd.DataFrame(zip(var_li,mod_fi),columns = ['Var','FI'])\n",
    "        res_df.sort_values('FI',ascending=False,inplace=True,ignore_index=True)\n",
    "        # Normalize to sum to 1\n",
    "        res_df['FI'] = res_df['FI']/res_df['FI'].sum()\n",
    "        return res_df\n",
    "\n",
    "# If you pass in multiple models\n",
    "# this will ensemble them, presumes regressor models\n",
    "class EnsMod():\n",
    "    def __init__(self, mods, av_func = 'mean'):\n",
    "        self.mods = mods #should be dict\n",
    "        self.av_func = av_func\n",
    "    def fit(self,X,weight=True,cat=True):\n",
    "        for key,mod in self.mods.items():\n",
    "            mod.fit(X,weight=weight,cat=cat)\n",
    "    def predict(self,X):\n",
    "        res = []\n",
    "        for key,mod in self.mods.items():\n",
    "            res.append(mod.predict(X))\n",
    "        res_df = pd.concat(res,axis=1)\n",
    "        if self.av_func == 'mean':\n",
    "            pred = res_df.mean(axis=1)\n",
    "        return pred\n",
    "    def predict_int(self,X):\n",
    "        pred = self.predict(X)\n",
    "        pred = pred.clip(1,5).round().astype(int)\n",
    "        return pred\n",
    "\n",
    "def rmse_region(data,pred='pred',true='severity',region='region',scale=False,ret=False):\n",
    "    dc = data[[region,true,pred]].copy()\n",
    "    if scale:\n",
    "        dc[pred] = strat(dc[pred])\n",
    "    dc[pred] = dc[pred].round().astype(int).clip(1,5)\n",
    "    dc[region] = dc[region].replace({1:'northeast',\n",
    "                                     2:'south',\n",
    "                                     3:'midwest',\n",
    "                                     4:'west'})\n",
    "    dc['sq_error'] = (dc[true] - dc[pred])**2\n",
    "    gr_val = dc.groupby(region,as_index=False)['sq_error'].mean()\n",
    "    gr_val['root_mse'] = np.sqrt(gr_val['sq_error'])\n",
    "    avg_error = gr_val['root_mse'].mean()\n",
    "    if ret:\n",
    "        regions = gr_val['region'].tolist()\n",
    "        regions.append('AvgError')\n",
    "        vals = gr_val['root_mse'].tolist()\n",
    "        vals.append(avg_error)\n",
    "        gr_di = {r:v for r,v in zip(regions,vals)}\n",
    "        return gr_di\n",
    "    else:\n",
    "        print(f'\\nAverage error {avg_error:.4f}')\n",
    "        print('\\nRegion Error')\n",
    "        print(gr_val[['region','root_mse']])\n",
    "\n",
    "def save_model(mod,name):\n",
    "    fname = f'./models/{name}.pkl'\n",
    "    outfile = open(fname,\"wb\")\n",
    "    pickle.dump(mod,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "def load_model(name):\n",
    "    fname = f'./models/{name}.pkl'\n",
    "    infile = open(fname, \"rb\")\n",
    "    mod = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return mod\n",
    "\n",
    "\n",
    "# function to check if similar to any past submissions\n",
    "def check_similar(current):\n",
    "    files = os.listdir(\"./submissions\")\n",
    "    for fi in files:\n",
    "        old = pd.read_csv(f\"./submissions/{fi}\")\n",
    "        dif = np.abs(current['severity'] - old['severity']).sum()\n",
    "        if dif == 0:\n",
    "            print(f'Date {fi} same as current')\n",
    "\n",
    "\n",
    "def check_day(current,day=\"sub_2023_01_31.csv\"):\n",
    "    old = pd.read_csv(fr\"./submissions/{day}\")\n",
    "    dstr = f'dif_{day[4:-4]}'\n",
    "    current[dstr] = old['severity'] - current['severity']\n",
    "    print(current[dstr].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src - feat.py\n",
    "\n",
    "def today_str():\n",
    "    now = datetime.now()\n",
    "    return now.strftime('%Y_%m_%d')\n",
    "\n",
    "\n",
    "reg_ord = {'west':4,\n",
    "           'midwest':3,\n",
    "           'south':2,\n",
    "           'northeast':1}\n",
    "\n",
    "# ordinal encoding for region\n",
    "def org_reg(data,rstr='region'):\n",
    "    data[rstr] = data[rstr].replace(reg_ord)\n",
    "\n",
    "def filter_reg(data, region):\n",
    "    return data.loc[data['region']==region]\n",
    "\n",
    "def ord_imtype(data,imstr='imtype'):\n",
    "    rep_di = {'land_sat':0,\n",
    "              'sentinel':1}\n",
    "    data[imstr] = data[imstr].fillna(-1).replace(rep_di)\n",
    "\n",
    "\n",
    "def filter_landsat(data):\n",
    "    rep_di = {'land_sat':0,\n",
    "              'sentinel':1}\n",
    "    data['imtype'] = data['imtype'].fillna(-1).replace(rep_di)\n",
    "    im_vars = ['prop_lake_500', 'r_500', 'g_500', 'b_500']\n",
    "    im_vars += ['prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']\n",
    "    im_vars += ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500']\n",
    "    im_vars += ['imtype']\n",
    "    landsat = data['imtype'] == 0\n",
    "    data.loc[landsat,im_vars] = -1\n",
    "\n",
    "def safesqrt(values):\n",
    "    return np.sqrt(values.clip(0))\n",
    "\n",
    "def safelog(x):\n",
    "    return np.log(x.clip(1))\n",
    "\n",
    "def strat(values):\n",
    "    edges = [np.NINF,20000,1e6,1e7,1e8,np.inf]\n",
    "    labs = [1,2,3,4,5]\n",
    "    res = pd.cut(values,bins=edges,labels=labs,right=False)\n",
    "    return res.astype(int)\n",
    "\n",
    "# Looking at train/test\n",
    "# there are a few clusters, want\n",
    "# to make sure to predict these well\n",
    "def cluster(x):\n",
    "    lat, lon = x[0], x[1]\n",
    "    if (lat < 41) & (lon < -116):\n",
    "        # cali\n",
    "        return 7\n",
    "    elif (lat < 41) & (lat > 36.29) & (lon < -92.9) & (lon > -102.2):\n",
    "        # midwest\n",
    "        return 6\n",
    "    elif (lat < 38.14) & (lat > 33.26) & (lon < -74.8) & (lon > -85.52):\n",
    "        # carolina\n",
    "        return 2\n",
    "    elif (lat < 43) & (lat > 38.7) & (lon < -75.4) & (lon > -83.55):\n",
    "        # erie\n",
    "        return 3\n",
    "    elif (lat < 43.1) & (lat > 40.7) & (lon < -69.5) & (lon > -74.6):\n",
    "        # mass\n",
    "        return 4\n",
    "    elif (lat < 49.6) & (lat > 41.5) & (lon < -83.55) & (lon > -104.56):\n",
    "        # dakota\n",
    "        return 1\n",
    "    else:\n",
    "        # other\n",
    "        return 5\n",
    "\n",
    "#                  1   2      2   3    3     4   4    5   5\n",
    "#te_st = pd.Series([1,20000,30000,1e6,1e6+1,1e7,1e7+1,1e8,1e9])\n",
    "#print(strat(te_st))\n",
    "\n",
    "db = './data/data.sqlite'\n",
    "\n",
    "train_query = \"\"\"\n",
    "SELECT \n",
    "  m.uid,\n",
    "  l.region,\n",
    "  l.severity,\n",
    "  l.density,\n",
    "  m.latitude,\n",
    "  m.longitude,\n",
    "  m.date,\n",
    "  e.elevation,\n",
    "  e.mine,\n",
    "  e.maxe,\n",
    "  e.dife,\n",
    "  e.avge,\n",
    "  e.stde,\n",
    "  sl.severity_100,\n",
    "  sl.logDensity_100,\n",
    "  sl.count_100,\n",
    "  sl.severity_300,\n",
    "  sl.logDensity_300,\n",
    "  sl.count_300,\n",
    "  sl.severity_1000,\n",
    "  sl.logDensity_1000,\n",
    "  sl.count_1000,\n",
    "  st.imtype,\n",
    "  st.prop_lake_500,\n",
    "  st.r_500,\n",
    "  st.g_500,\n",
    "  st.b_500,\n",
    "  st.prop_lake_1000,\n",
    "  st.r_1000,\n",
    "  st.g_1000,\n",
    "  st.b_1000,\n",
    "  st.prop_lake_2500,\n",
    "  st.r_2500,\n",
    "  st.g_2500,\n",
    "  st.b_2500\n",
    "FROM meta AS m\n",
    "LEFT JOIN elevation_dem AS e\n",
    "  ON m.uid = e.uid\n",
    "LEFT JOIN spat_lag AS sl\n",
    "  ON m.uid = sl.uid\n",
    "LEFT JOIN sat AS st\n",
    "  ON m.uid = st.uid\n",
    "LEFT JOIN labels AS l\n",
    "  ON m.uid = l.uid\n",
    "WHERE\n",
    "  m.split = 'train'\n",
    "\"\"\"\n",
    "\n",
    "test_query = \"\"\"\n",
    "SELECT \n",
    "  m.uid,\n",
    "  l.region,\n",
    "  m.latitude,\n",
    "  m.longitude,\n",
    "  m.date,\n",
    "  e.elevation,\n",
    "  e.mine,\n",
    "  e.maxe,\n",
    "  e.dife,\n",
    "  e.avge,\n",
    "  e.stde,\n",
    "  sl.severity_100,\n",
    "  sl.logDensity_100,\n",
    "  sl.count_100,\n",
    "  sl.severity_300,\n",
    "  sl.logDensity_300,\n",
    "  sl.count_300,\n",
    "  sl.severity_1000,\n",
    "  sl.logDensity_1000,\n",
    "  sl.count_1000,\n",
    "  st.imtype,\n",
    "  st.prop_lake_500,\n",
    "  st.r_500,\n",
    "  st.g_500,\n",
    "  st.b_500,\n",
    "  st.prop_lake_1000,\n",
    "  st.r_1000,\n",
    "  st.g_1000,\n",
    "  st.b_1000,\n",
    "  st.prop_lake_2500,\n",
    "  st.r_2500,\n",
    "  st.g_2500,\n",
    "  st.b_2500\n",
    "FROM meta AS m\n",
    "LEFT JOIN elevation_dem AS e\n",
    "  ON m.uid = e.uid\n",
    "LEFT JOIN spat_lag AS sl\n",
    "  ON m.uid = sl.uid\n",
    "LEFT JOIN sat AS st\n",
    "  ON m.uid = st.uid\n",
    "LEFT JOIN format AS l\n",
    "  ON m.uid = l.uid\n",
    "WHERE\n",
    "  m.split = 'test'\n",
    "\"\"\"\n",
    "\n",
    "def add_table(data,tab_name,db_str=db):\n",
    "    db_con = sqlite3.connect(db_str)\n",
    "    dn = data.copy()\n",
    "    dn['DateTime'] = pd.to_datetime('now',utc=True)\n",
    "    dn.to_sql(tab_name,index=False,if_exists='replace',con=db_con)\n",
    "\n",
    "\n",
    "def get_both(db_str=db,split_pred=False):\n",
    "    r1 = get_data('train',db_str,split_pred)\n",
    "    r1['test'] = 0\n",
    "    r1.drop(columns=['severity','density','logDensity'],inplace=True)\n",
    "    r2 = get_data('test',db_str,split_pred)\n",
    "    r2['test'] = 1\n",
    "    res_df = pd.concat([r1,r2],axis=0)\n",
    "    return res_df.reset_index(drop=True)\n",
    "\n",
    "def get_data(region, data_type='train',db_str=db,split_pred=False):\n",
    "    db_con = sqlite3.connect(db_str)\n",
    "    if data_type == 'train':\n",
    "        sql = train_query\n",
    "    elif data_type == 'test':\n",
    "        sql = test_query\n",
    "    dat = pd.read_sql(sql,con=db_con)\n",
    "    org_reg(dat) # Region ordinal encode\n",
    "    # Winning solution used landsat-7 data\n",
    "    #ord_imtype(dat) # image type landsat/sentinel\n",
    "    filter_landsat(dat) # filtering mistake landsat-7 info\n",
    "    dat = dat.fillna(-1) # missing a bit of sat data\n",
    "    dat['cluster'] = dat[['latitude','longitude']].apply(cluster,axis=1)\n",
    "    dat = filter_reg(dat, region)\n",
    "    if data_type == 'train':\n",
    "        dat['logDensity'] = safelog(dat['density'])\n",
    "    if split_pred:\n",
    "        pred_test = pd.read_sql('SELECT uid, pred AS split_pred FROM split_pred',con=db_con)\n",
    "        dat = dat.merge(pred_test,on='uid')\n",
    "    return dat\n",
    "\n",
    "\n",
    "# Need logic to take predictions and get them in the right order\n",
    "def sub_format(data,pred='pred'):\n",
    "    form = pd.read_csv('./data/submission_format.csv')\n",
    "    # some logic to transform predictions via Duan\n",
    "    # smearing\n",
    "    dp = data[[pred,'uid']].copy()\n",
    "    dp[pred] = dp[pred].round().astype(int).clip(1,5)\n",
    "    mf = form.merge(dp,on='uid')\n",
    "    mf['severity'] = mf['pred']\n",
    "    return mf[['uid','region','severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src - get_data.py\n",
    "'''\n",
    "Functions to get different data sources\n",
    "and cache them in sqllite DB\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from io import StringIO\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from math import sin, cos, tan, acos, atan, atan2, radians, sqrt, pi\n",
    "import rioxarray\n",
    "import planetary_computer as pc\n",
    "import geopy.distance as distance\n",
    "import odc.stac\n",
    "#import cv2\n",
    "\n",
    "\n",
    "db = './data/data.sqlite'\n",
    "db_con = sqlite3.connect(db)\n",
    "\n",
    "# having a hard time installing opencv on\n",
    "# planetary computer, only use this function\n",
    "# so far\n",
    "\n",
    "def cv2_norm(matrix):\n",
    "    amin = matrix.min()\n",
    "    amax = matrix.max()\n",
    "    modif = amax/(amax - amin)\n",
    "    shift_mat = np.round((matrix - amin)*modif)\n",
    "    return shift_mat.astype(int)\n",
    "\n",
    "\n",
    "# chunking pandas dataframe\n",
    "def chunk_pd(df, chunk_size):\n",
    "    nrows = df.shape[0]\n",
    "    tot_split = np.ceil(nrows/chunk_size)\n",
    "    split_rows = np.array_split(np.arange(nrows), tot_split)\n",
    "    fin_list_df = [df.iloc[s, :] for s in split_rows]\n",
    "    return fin_list_df\n",
    "\n",
    "# Get table names\n",
    "def get_table_names(con=db_con):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    res = pd.read_sql(query,con)\n",
    "    return res['name'].tolist()\n",
    "\n",
    "# Drop table if needed\n",
    "def drop_table(table,con=db_con):\n",
    "    dt = f'DROP TABLE {table}'\n",
    "    con.execute(dt)\n",
    "\n",
    "# Check if table exists\n",
    "def tab_exists(table,con=db_con):\n",
    "    rt = get_table_names(con)\n",
    "    res = table in rt\n",
    "    return res\n",
    "\n",
    "# Add a table with DateTime appended\n",
    "def add_table(data,tab_name,con=db_con):\n",
    "    dn = data.copy()\n",
    "    dn['DateTime'] = pd.to_datetime('now',utc=True)\n",
    "    dn.to_sql(tab_name,index=False,if_exists='append',con=con)\n",
    "\n",
    "# Seeing to update data based on old table\n",
    "def get_update(data,table_name,con=db_con):\n",
    "    query = f'SELECT DISTINCT uid FROM {table_name};'\n",
    "    uids = pd.read_sql(query,con=con)['uid'].tolist()\n",
    "    notin_old = ~data['uid'].isin(uids)\n",
    "    return data[notin_old].copy()\n",
    "\n",
    "# Adds in competition meta data\n",
    "def add_meta_data(con=db_con):\n",
    "    ret_dat = {}\n",
    "    name_map = {'meta': './data/metadata.csv',\n",
    "                'labels': './data/train_labels.csv',\n",
    "                'format': './data/submission_format.csv'}\n",
    "    for n,l in name_map.items():\n",
    "        if tab_exists(n):\n",
    "            query = f'SELECT * from {n}'\n",
    "            d = pd.read_sql(query,con=con)\n",
    "            ret_dat[n] = d\n",
    "        else:\n",
    "            d = pd.read_csv(l)\n",
    "            add_table(d,n,con)\n",
    "            ret_dat[n] = d\n",
    "    return ret_dat\n",
    "\n",
    "# Just to make sure those tables are populated\n",
    "meta_data = add_meta_data()\n",
    "\n",
    "###########################################################################################\n",
    "# VINCENTY FORMULA FOR DISTANCE BETWEEN LAT/LON\n",
    "\n",
    "# The function below is  based on transcribing and adapting code from\n",
    "# http://www.movable-type.co.uk/scripts/latlong-vincenty.html\n",
    "# which includes the note\n",
    "#I offer these formulae & scripts for free use and adaptation as my contribution to the open-source \n",
    "#info-sphere from which I have received so much. You are welcome to re-use these scripts \n",
    "#[under a simple attribution license, without any warranty express or implied] \n",
    "#provided solely that you retain my copyright notice and a link to this page.[above]\n",
    "#If you have any queries or find any problems, contact me at ku.oc.epyt-elbavom@oeg-stpircs.\n",
    "#\n",
    "#(c) 2002-2011 Chris Veness \n",
    "\n",
    "#Another useful source is\n",
    "#http://search.cpan.org/~bluefeet/GIS-Distance-0.01001/lib/GIS/Distance/Vincenty.pm\n",
    "\n",
    "# ellipsoid parameters\n",
    "class Elp:\n",
    "    a = 6378137.\n",
    "    b = 6356752.314245\n",
    "    f = (a-b) / a\n",
    "\n",
    "def ellipseDist(lat1, lon1, lat2, lon2, inradians=False):\n",
    "    \"\"\"Return the distance in meters between the two sets of coordinates  using an ellipsoidal approximation to the earth \n",
    "    \n",
    "    Calculation uses Vincenty inverse ellipsoid formula with WGS-84 ellipsoid parameters.\n",
    "    \n",
    "    lat1 and lon1 are the latitude and longitude of the first point and lat2, lon2 of the second.\n",
    "    if inradians is True, locations are in radians; otherwise they are in degrees.\n",
    "    If the algorithm fails to converge, the return value is None, which will become SYSMIS when returned to SPSS\n",
    "    \n",
    "    The ellipsoid model is more accurrate than the simpler spherical approximation.\"\"\"\n",
    "    \n",
    "    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:\n",
    "        return None\n",
    "    if not inradians:\n",
    "        lat1, lon1, lat2, lon2 = [radians(ang) for ang in [lat1, lon1, lat2, lon2]]\n",
    "    u1, u2 = [atan((1-Elp.f) * tan(x)) for x in [lat1, lat2]]\n",
    "    sinu1, sinu2 = sin(u1), sin(u2)\n",
    "    cosu1, cosu2 = cos(u1), cos(u2)\n",
    "    londif = lon2 - lon1\n",
    "    \n",
    "    lam = londif\n",
    "    lamp = 2 * pi\n",
    "    numiter = 50  # maximum number of iterations\n",
    "    \n",
    "    for i in range(numiter):\n",
    "        if abs(lam - lamp) <= 1e-12:\n",
    "            break\n",
    "        sinLam = sin(lam)\n",
    "        cosLam = cos(lam)\n",
    "        sinSigma = sqrt((cosu2 * sinLam) **2 + (cosu1 * sinu2-sinu1 * cosu2 * cosLam)**2)\n",
    "        if sinSigma == 0:\n",
    "            return 0\n",
    "        cosSigma = sinu1 * sinu2 + cosu1 * cosu2 * cosLam\n",
    "        sigma = atan2(sinSigma, cosSigma)\n",
    "        sinAlpha = cosu1 * cosu2 * sinLam / sinSigma\n",
    "        cosSqAlpha = 1 - sinAlpha **2\n",
    "        try:\n",
    "            cos2SigmaM = cosSigma - 2 * sinu1 * sinu2 / cosSqAlpha\n",
    "        except:\n",
    "            cos2SigmaM = 0 # equatorial line\n",
    "        c = (Elp.f/16) * cosSqAlpha * (4 + Elp.f * (4 - 3 * cosSqAlpha))\n",
    "        lamp = lam\n",
    "        lam = londif + (1 - c)  * Elp.f * sinAlpha\\\n",
    "            * (sigma + c * sinSigma* (cos2SigmaM + c * cosSigma * (-1 + 2 * cos2SigmaM **2)))\n",
    "    else:\n",
    "            return None\n",
    "        \n",
    "    uSq = cosSqAlpha * (Elp.a * Elp.a - Elp.b * Elp.b) / (Elp.b * Elp.b)\n",
    "    A = 1 + uSq / 16384 * (4096+uSq * (-768 + uSq * (320-175 * uSq)))\n",
    "    B = uSq/1024 * (256 + uSq * (-128 + uSq * (74-47 * uSq)))\n",
    "    deltaSigma = B * sinSigma * (cos2SigmaM + B / 4 * (cosSigma * (-1+2 * cos2SigmaM **2)- \\\n",
    "        B / 6 * cos2SigmaM * (-3 + 4 * sinSigma **2) * (-3 + 4 * cos2SigmaM **2)))\n",
    "    s = Elp.b * A * (sigma - deltaSigma)\n",
    "    return s\n",
    "\n",
    "# need to apply this over a vector, makes it ok with pandas dataframe\n",
    "# returns in kilometers now\n",
    "def vector_vincent(x, lat2, lon2):\n",
    "    return ellipseDist(x[0], x[1], lat2, lon2, inradians=False)/1000\n",
    "\n",
    "#lat = [35,36,37,38]\n",
    "#lon = [-72,-71,-72,-71]\n",
    "#test_df = pd.DataFrame(zip(lat,lon),columns=['lat','lon'])\n",
    "#dist = test_df.apply(vector_vincent,args=(35.1,-72.1),axis=1)\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "# Average outcome in data from sites within d kilometers (default 300)\n",
    "# Limit to only those before outcome\n",
    "def spatial_lag(pred_data,data,distance=[100,300,1000],fields=['severity','logDensity'],lat='latitude',lon='longitude',date='date'):\n",
    "    res = [] # final list with info\n",
    "    # prepping data to check\n",
    "    dc = data[fields + [lat,lon,date]].copy()\n",
    "    dc[date] = pd.to_datetime(dc[date])\n",
    "    # now prepping pred_data\n",
    "    plon_li = pred_data[lon].tolist()\n",
    "    plat_li = pred_data[lat].tolist()\n",
    "    pdat_li = pd.to_datetime(pred_data[date]).tolist()\n",
    "    for plon,plat,pdat in zip(plon_li,plat_li,pdat_li):\n",
    "        loc_dat = []\n",
    "        count_dat = []\n",
    "        sub_dc = dc[dc[date] < pdat].copy()\n",
    "        km_dist = sub_dc[[lat,lon]].apply(vector_vincent,args=(plat,plon),axis=1)\n",
    "        for d in distance:\n",
    "            sub_dist = sub_dc[km_dist < d].copy()\n",
    "            count_dat.append(sub_dist.shape[0])\n",
    "            for v in fields:\n",
    "                loc_dat.append(sub_dist[v].mean())\n",
    "        res.append(loc_dat + count_dat)\n",
    "    res_labs = []\n",
    "    for d in distance:\n",
    "        for v in fields:\n",
    "            res_labs.append(f'{v}_{str(d)}')\n",
    "    res_labs += [f'count_{str(d)}' for d in distance]\n",
    "    res_pd = pd.DataFrame(res,columns=res_labs,index=pred_data.index)\n",
    "    return res_pd.fillna(-1)\n",
    "\n",
    "\n",
    "def get_spatiallag(con=db_con,table_name='spat_lag'):\n",
    "    # get full metadata table\n",
    "    full_dat = pd.read_csv('./data/metadata.csv')\n",
    "    # get metadata with only labels\n",
    "    labs = pd.read_csv('./data/train_labels.csv')\n",
    "    only_lab = labs.merge(full_dat,on='uid')\n",
    "    only_lab['logDensity'] = np.log(only_lab['density'].clip(1))\n",
    "    # create spatial lag\n",
    "    lag_df = spatial_lag(full_dat,only_lab)\n",
    "    lag_df['uid'] = full_dat['uid']\n",
    "    # save to a new table\n",
    "    add_table(lag_df,table_name)\n",
    "    return lag_df\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# helper functions from DataDriven post\n",
    "# https://drivendata.co/blog/tick-tick-bloom-benchmark\n",
    "\n",
    "\n",
    "def get_bounding_box(latitude, longitude, meter_buffer=1000):\n",
    "    \"\"\"\n",
    "    Given a latitude, longitude, and buffer in meters, returns a bounding\n",
    "    box around the point with the buffer on the left, right, top, and bottom.\n",
    "    Returns a list of [minx, miny, maxx, maxy]\n",
    "    \"\"\"\n",
    "    distance_search = distance.distance(meters=meter_buffer)\n",
    "    # calculate the lat/long bounds based on ground distance\n",
    "    # bearings are cardinal directions to move (south, west, north, and east)\n",
    "    min_lat = distance_search.destination((latitude, longitude), bearing=180)[0]\n",
    "    min_long = distance_search.destination((latitude, longitude), bearing=270)[1]\n",
    "    max_lat = distance_search.destination((latitude, longitude), bearing=0)[0]\n",
    "    max_long = distance_search.destination((latitude, longitude), bearing=90)[1]\n",
    "    return [min_long, min_lat, max_long, max_lat]\n",
    "\n",
    "\n",
    "# get our date range to search, and format correctly for query\n",
    "def get_date_range(date, time_buffer_days=15):\n",
    "    \"\"\"Get a date range to search for in the planetary computer based\n",
    "    on a sample's date. The time range will include the sample date\n",
    "    and time_buffer_days days prior\n",
    "    Returns a string\"\"\"\n",
    "    datetime_format = \"%Y-%m-%dT\"\n",
    "    range_start = pd.to_datetime(date) - timedelta(days=time_buffer_days)\n",
    "    date_range = f\"{range_start.strftime(datetime_format)}/{pd.to_datetime(date).strftime(datetime_format)}\"\n",
    "    return date_range\n",
    "\n",
    "\n",
    "def crop_sentinel_image(item, bounding_box):\n",
    "    \"\"\"\n",
    "    Given a STAC item from Sentinel-2 and a bounding box tuple in the format\n",
    "    (minx, miny, maxx, maxy), return a cropped portion of the item's visual\n",
    "    imagery in the bounding box.\n",
    "    Returns the image as a numpy array with dimensions (color band, height, width)\n",
    "    \"\"\"\n",
    "    (minx, miny, maxx, maxy) = bounding_box\n",
    "\n",
    "    image = rioxarray.open_rasterio(pc.sign(item.assets[\"visual\"].href)).rio.clip_box(\n",
    "        minx=minx,\n",
    "        miny=miny,\n",
    "        maxx=maxx,\n",
    "        maxy=maxy,\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    # Should I return X/Y as well?\n",
    "    return cv2_norm(image.to_numpy())\n",
    "\n",
    "\n",
    "def crop_landsat_image(item, bounding_box):\n",
    "    \"\"\"\n",
    "    Given a STAC item from Landsat and a bounding box tuple in the format\n",
    "    (minx, miny, maxx, maxy), return a cropped portion of the item's visual\n",
    "    imagery in the bounding box.\n",
    "    Returns the image as a numpy array with dimensions (color band, height, width)\n",
    "    \"\"\"\n",
    "    (minx, miny, maxx, maxy) = bounding_box\n",
    "\n",
    "    image = odc.stac.stac_load(\n",
    "        [pc.sign(item)], bands=[\"red\", \"green\", \"blue\"], bbox=[minx, miny, maxx, maxy]\n",
    "    ).isel(time=0)\n",
    "    image_array = image[[\"red\", \"green\", \"blue\"]].to_array().to_numpy()\n",
    "\n",
    "    # normalize to 0 - 255 values\n",
    "    #image_array = cv2.normalize(image_array, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    image_array = cv2_norm(image_array)\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing script to prep data in sqlite\n",
      "This expects the csv files listed at front\n",
      "of script to be in the data folder\n",
      "Resulting table names in sqlite\n",
      "['meta', 'labels', 'format', 'elevation_dem', 'split_pred', 'spat_lag', 'sat']\n"
     ]
    }
   ],
   "source": [
    "# main_prepdata.py\n",
    "'''\n",
    "So you can either do it two ways\n",
    "One is to run each of the files\n",
    "individually, e.g.\n",
    "cd ./algaebloom\n",
    "python get_dem.py\n",
    "python get_lag.py\n",
    "python get_sat.py\n",
    "These take along time, and dem/sat can\n",
    "fail periodically\n",
    "They are not idempotent, so incrementally save \n",
    "the values to a SQLite database (in the data folder)\n",
    "The easier way though to replicate the SQLite database\n",
    "though is to run this script, it presumes you have the files\n",
    "algaebloom/data\n",
    "       - metadata.csv           # from competition\n",
    "       - train_labels.csv       # from competition\n",
    "       - submission_format.csv  # from competition\n",
    "       - elevation_dem.csv      # generated via get_dem.py\n",
    "       - spat_lag.csv           # generated via get_lag.py\n",
    "       - sat.csv                # generated via get_sat.py\n",
    "       - split_pred.csv         # generated via get_split.py\n",
    "The final solution does not use the lag values, but have\n",
    "included to replicate my prior tests. Get split needs to be\n",
    "run AFTER the elevation stats are prepped, but otherwise \n",
    "the order does not matter\n",
    "any questions? Feel free to email me, \n",
    "apwheele@gmail.com\n",
    "Andy Wheeler\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "db = './data/data.sqlite'\n",
    "tab_names = ['elevation_dem','split_pred','spat_lag','sat']\n",
    "meta_names = ['meta','labels', 'format']\n",
    "meta_csv = ['metadata','train_labels','submission_format']\n",
    "\n",
    "fd = {c:t for c,t in zip(meta_csv,meta_names)}\n",
    "ft = {t:t for t in tab_names}\n",
    "fd.update(ft)\n",
    "\n",
    "# Function to save out csv files\n",
    "def save_csv(db=db):\n",
    "    db_con = sqlite3.connect(db)\n",
    "    for t in tab_names:\n",
    "        res = pd.read_sql(f'SELECT * FROM {t}',db_con)\n",
    "        res.to_csv(f'./data/{t}.csv',index=False)\n",
    "\n",
    "# Function to prep SQLite DB\n",
    "def prep_sql(db=db):\n",
    "    db_con = sqlite3.connect(db)\n",
    "    for csv,tab_name in fd.items():\n",
    "        res = pd.read_csv(f'./data/{csv}.csv')\n",
    "        res.to_sql(tab_name,index=False,if_exists='replace',con=db_con)\n",
    "    # Showing resulting table names\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    rt = pd.read_sql(query,db_con)\n",
    "    print('Resulting table names in sqlite')\n",
    "    print(rt['name'].tolist())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('Executing script to prep data in sqlite')\n",
    "    print('This expects the csv files listed at front')\n",
    "    print('of script to be in the data folder')\n",
    "    prep_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "res_results = {}\n",
    "\n",
    "# Setting up data and variable sets\n",
    "\n",
    "train_dat = feat.get_data(split_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fit with weights\n",
    "weight_cats = tuple([False])\n",
    "\n",
    "# Model fit with categorical variables\n",
    "cat_cats = (True,False)\n",
    "\n",
    "# Lat/Lon included in model\n",
    "xy_cats = {'no': [],\n",
    "           'both': ['latitude','longitude'],\n",
    "           'lat': ['latitude'],\n",
    "           'lon': ['longitude']}\n",
    "\n",
    "xy_keys = tuple(xy_cats.keys())\n",
    "\n",
    "# Region variables\n",
    "region_cats = {'both': ['region','cluster'],\n",
    "               'reg':  ['region'],\n",
    "               'clust':  ['cluster']}\n",
    "\n",
    "reg_keys = tuple(region_cats.keys())\n",
    "\n",
    "# Elevation Variables\n",
    "ele_cats = {'max_dif':['maxe','dife'],\n",
    "            'all_var':['maxe','dife','elevation','stde'],\n",
    "            'ele_std':['elevation','stde'],\n",
    "            'ele_dif':['elevation','dife'],\n",
    "            'max_std':['maxe','stde'] }\n",
    "\n",
    "ele_keys = tuple(ele_cats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Lag Variables\n",
    "sl_cats = {#'lag100': ['severity_100','logDensity_100','count_100'],\n",
    "           #'lag300': ['severity_300','logDensity_300','count_300'],\n",
    "           #'lag1000': ['severity_1000','logDensity_1000','count_1000'],\n",
    "           'lagNone': []}\n",
    "\n",
    "sl_keys = tuple(sl_cats.keys())\n",
    "\n",
    "# Sat imagery data\n",
    "sat_cats = {'sat500': ['imtype','prop_lake_500','r_500','g_500','b_500'],\n",
    "            'sat1000': ['imtype','prop_lake_1000','r_1000','g_1000','b_1000'],\n",
    "            'sat2500': ['imtype','prop_lake_2500','r_2500','g_2500','b_2500'],\n",
    "            'sat500_1000': ['imtype','prop_lake_500','r_500','g_500','b_500', 'prop_lake_1000','r_1000','g_1000','b_1000'],\n",
    "            'sat500_2500': ['imtype','prop_lake_500','r_500','g_500','b_500', 'prop_lake_2500','r_2500','g_2500','b_2500'],\n",
    "            'sat1000_2500': ['imtype','prop_lake_1000','r_1000','g_1000','b_1000', 'prop_lake_2500','r_2500','g_2500','b_2500']}\n",
    "\n",
    "sat_keys = tuple(sat_cats.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-14 19:58:50,330]\u001b[0m A new study created in memory with name: no-name-3260d55f-e0c4-47ca-937b-00d0582b473f\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 19:59:07,744]\u001b[0m Trial 0 finished with value: 0.8079305712392552 and parameters: {'n_estimators': 230, 'max_depth': 4, 'ele_vars': 'ele_std', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': True, 'sat_set': 'sat500'}. Best is trial 0 with value: 0.8079305712392552.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 19:59:30,775]\u001b[0m Trial 1 finished with value: 0.8275873216645457 and parameters: {'n_estimators': 460, 'max_depth': 3, 'ele_vars': 'all_var', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': True, 'sat_set': 'sat500_2500'}. Best is trial 0 with value: 0.8079305712392552.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:00:06,758]\u001b[0m Trial 2 finished with value: 0.8235443492503702 and parameters: {'n_estimators': 260, 'max_depth': 6, 'ele_vars': 'all_var', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat500'}. Best is trial 0 with value: 0.8079305712392552.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:00:49,857]\u001b[0m Trial 3 finished with value: 0.7965233369996939 and parameters: {'n_estimators': 580, 'max_depth': 4, 'ele_vars': 'all_var', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_1000'}. Best is trial 3 with value: 0.7965233369996939.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:01:57,526]\u001b[0m Trial 4 finished with value: 0.7961742876191721 and parameters: {'n_estimators': 540, 'max_depth': 6, 'ele_vars': 'max_std', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000'}. Best is trial 4 with value: 0.7961742876191721.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:02:39,711]\u001b[0m Trial 5 finished with value: 0.82576685879551 and parameters: {'n_estimators': 310, 'max_depth': 9, 'ele_vars': 'all_var', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_1000'}. Best is trial 4 with value: 0.7961742876191721.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:03:02,498]\u001b[0m Trial 6 finished with value: 0.7931923995557597 and parameters: {'n_estimators': 240, 'max_depth': 7, 'ele_vars': 'max_dif', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:04:05,291]\u001b[0m Trial 7 finished with value: 0.8090085473947782 and parameters: {'n_estimators': 550, 'max_depth': 9, 'ele_vars': 'max_dif', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_1000'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:05:29,714]\u001b[0m Trial 8 finished with value: 0.8265504452658705 and parameters: {'n_estimators': 500, 'max_depth': 8, 'ele_vars': 'ele_dif', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_2500'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:05:32,062]\u001b[0m Trial 9 finished with value: 0.9141330303728592 and parameters: {'n_estimators': 30, 'max_depth': 3, 'ele_vars': 'ele_std', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:05:49,128]\u001b[0m Trial 10 finished with value: 0.8205566340556774 and parameters: {'n_estimators': 80, 'max_depth': 7, 'ele_vars': 'max_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:06:38,648]\u001b[0m Trial 11 finished with value: 0.7963954513356124 and parameters: {'n_estimators': 410, 'max_depth': 6, 'ele_vars': 'max_std', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:06:54,029]\u001b[0m Trial 12 finished with value: 0.8057605278379224 and parameters: {'n_estimators': 160, 'max_depth': 5, 'ele_vars': 'max_std', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000_2500'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:07:36,598]\u001b[0m Trial 13 finished with value: 0.8064165008157997 and parameters: {'n_estimators': 360, 'max_depth': 7, 'ele_vars': 'max_std', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000'}. Best is trial 6 with value: 0.7931923995557597.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:07:54,978]\u001b[0m Trial 14 finished with value: 0.7813678403120428 and parameters: {'n_estimators': 170, 'max_depth': 7, 'ele_vars': 'max_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000'}. Best is trial 14 with value: 0.7813678403120428.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:08:13,069]\u001b[0m Trial 15 finished with value: 0.7947798711733387 and parameters: {'n_estimators': 150, 'max_depth': 10, 'ele_vars': 'max_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000'}. Best is trial 14 with value: 0.7813678403120428.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:08:37,135]\u001b[0m Trial 16 finished with value: 0.7860224612019721 and parameters: {'n_estimators': 180, 'max_depth': 8, 'ele_vars': 'max_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 14 with value: 0.7813678403120428.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:08:55,498]\u001b[0m Trial 17 finished with value: 0.7872676887575679 and parameters: {'n_estimators': 110, 'max_depth': 8, 'ele_vars': 'max_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 14 with value: 0.7813678403120428.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:09:25,630]\u001b[0m Trial 18 finished with value: 0.7745133837856714 and parameters: {'n_estimators': 200, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 18 with value: 0.7745133837856714.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:10:12,000]\u001b[0m Trial 19 finished with value: 0.7769622641330731 and parameters: {'n_estimators': 320, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 18 with value: 0.7745133837856714.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:11:00,110]\u001b[0m Trial 20 finished with value: 0.777091689314591 and parameters: {'n_estimators': 330, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 18 with value: 0.7745133837856714.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:11:46,225]\u001b[0m Trial 21 finished with value: 0.768072820483889 and parameters: {'n_estimators': 320, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:12:36,650]\u001b[0m Trial 22 finished with value: 0.7914768394176027 and parameters: {'n_estimators': 390, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:13:20,904]\u001b[0m Trial 23 finished with value: 0.7751606018608961 and parameters: {'n_estimators': 290, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:13:55,446]\u001b[0m Trial 24 finished with value: 0.7787535583099217 and parameters: {'n_estimators': 270, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:14:26,562]\u001b[0m Trial 25 finished with value: 0.7884320338784027 and parameters: {'n_estimators': 210, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:15:13,716]\u001b[0m Trial 26 finished with value: 0.7894684102867665 and parameters: {'n_estimators': 430, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:15:47,068]\u001b[0m Trial 27 finished with value: 0.7751771792050938 and parameters: {'n_estimators': 290, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:16:31,362]\u001b[0m Trial 28 finished with value: 0.77774625783735 and parameters: {'n_estimators': 380, 'max_depth': 8, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:17:02,852]\u001b[0m Trial 29 finished with value: 0.7800361675257401 and parameters: {'n_estimators': 230, 'max_depth': 10, 'ele_vars': 'ele_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:17:16,774]\u001b[0m Trial 30 finished with value: 0.7917479991653184 and parameters: {'n_estimators': 100, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:17:50,456]\u001b[0m Trial 31 finished with value: 0.7783736038279453 and parameters: {'n_estimators': 280, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:18:35,627]\u001b[0m Trial 32 finished with value: 0.788696356485362 and parameters: {'n_estimators': 350, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:19:10,213]\u001b[0m Trial 33 finished with value: 0.7853298831430454 and parameters: {'n_estimators': 290, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:19:14,408]\u001b[0m Trial 34 finished with value: 0.9187497114370254 and parameters: {'n_estimators': 200, 'max_depth': 2, 'ele_vars': 'ele_dif', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:20:07,382]\u001b[0m Trial 35 finished with value: 0.7794484988263908 and parameters: {'n_estimators': 450, 'max_depth': 8, 'ele_vars': 'ele_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:20:36,235]\u001b[0m Trial 36 finished with value: 0.8212369635412138 and parameters: {'n_estimators': 230, 'max_depth': 10, 'ele_vars': 'all_var', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:21:06,179]\u001b[0m Trial 37 finished with value: 0.8237081973918604 and parameters: {'n_estimators': 260, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:21:29,207]\u001b[0m Trial 38 finished with value: 0.7954305496673821 and parameters: {'n_estimators': 300, 'max_depth': 5, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_1000'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:22:12,072]\u001b[0m Trial 39 finished with value: 0.8012874486689547 and parameters: {'n_estimators': 340, 'max_depth': 8, 'ele_vars': 'ele_dif', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:22:44,820]\u001b[0m Trial 40 finished with value: 0.7928878533026006 and parameters: {'n_estimators': 250, 'max_depth': 9, 'ele_vars': 'all_var', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': True, 'sat_set': 'sat500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:23:27,181]\u001b[0m Trial 41 finished with value: 0.7822792330140315 and parameters: {'n_estimators': 320, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:24:20,764]\u001b[0m Trial 42 finished with value: 0.7754774774487619 and parameters: {'n_estimators': 370, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:25:28,083]\u001b[0m Trial 43 finished with value: 0.7824835926152482 and parameters: {'n_estimators': 470, 'max_depth': 10, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:26:16,546]\u001b[0m Trial 44 finished with value: 0.7776879081405971 and parameters: {'n_estimators': 380, 'max_depth': 9, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:27:35,555]\u001b[0m Trial 45 finished with value: 0.8262417711664691 and parameters: {'n_estimators': 360, 'max_depth': 10, 'ele_vars': 'ele_std', 'xy_set': 'no', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_1000'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:28:36,219]\u001b[0m Trial 46 finished with value: 0.8001289326756389 and parameters: {'n_estimators': 300, 'max_depth': 9, 'ele_vars': 'max_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:29:47,090]\u001b[0m Trial 47 finished with value: 0.7945110128219421 and parameters: {'n_estimators': 420, 'max_depth': 8, 'ele_vars': 'all_var', 'xy_set': 'lon', 'sl_set': 'lagNone', 'reg_set': 'both', 'weight': False, 'cat_type': False, 'sat_set': 'sat2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:30:09,805]\u001b[0m Trial 48 finished with value: 0.8026907179108222 and parameters: {'n_estimators': 140, 'max_depth': 7, 'ele_vars': 'ele_dif', 'xy_set': 'lat', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': False, 'sat_set': 'sat500_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:30:21,935]\u001b[0m Trial 49 finished with value: 0.7867605830655765 and parameters: {'n_estimators': 190, 'max_depth': 4, 'ele_vars': 'ele_dif', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'reg', 'weight': False, 'cat_type': True, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n",
      "\u001b[32m[I 2023-04-14 20:31:22,143]\u001b[0m Trial 50 finished with value: 0.7939699394691739 and parameters: {'n_estimators': 490, 'max_depth': 6, 'ele_vars': 'max_std', 'xy_set': 'both', 'sl_set': 'lagNone', 'reg_set': 'clust', 'weight': False, 'cat_type': False, 'sat_set': 'sat1000_2500'}. Best is trial 21 with value: 0.768072820483889.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective_lgb(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 20, 600, 10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"ele_set\": trial.suggest_categorical(\"ele_vars\", ele_keys),\n",
    "        \"xy_set\": trial.suggest_categorical(\"xy_set\", xy_keys),\n",
    "        \"sl_set\": trial.suggest_categorical(\"sl_set\", sl_keys),\n",
    "        \"reg_set\": trial.suggest_categorical(\"reg_set\", reg_keys),\n",
    "        \"weight\": trial.suggest_categorical(\"weight\", weight_cats),\n",
    "        \"cat_type\": trial.suggest_categorical(\"cat_type\", cat_cats),\n",
    "        \"sat_set\": trial.suggest_categorical(\"sat_set\", sat_keys),\n",
    "    }\n",
    "    # Setting the different variables\n",
    "    ov = region_cats[param['reg_set']]\n",
    "    #if 'imtype' in sat_cats[param['sat_set']]:\n",
    "    #    ov.append('imtype')\n",
    "    cv = ele_cats[param['ele_set']] + xy_cats[param['xy_set']]\n",
    "    cv += sl_cats[param['sl_set']]\n",
    "    cv += sat_cats[param['sat_set']]\n",
    "    rm = mod.RegMod(ord_vars=ov,\n",
    "                    dum_vars=None,\n",
    "                    dat_vars=['date'],\n",
    "                    ide_vars=cv,\n",
    "                    weight = 'split_pred',\n",
    "                    y='severity',\n",
    "                    mod = mod.LGBMRegressor(n_estimators=round(param['n_estimators']),\n",
    "                                            max_depth=param['max_depth']))\n",
    "    avg_rmse = rm.met_eval(train_dat,ret=True,weight=param['weight'],cat=param['cat_type'])\n",
    "    return avg_rmse\n",
    "\n",
    "study_lgb = optuna.create_study(direction=\"minimize\")\n",
    "study_lgb.optimize(objective_lgb, n_trials=300) # 150\n",
    "trial_lgb = study_lgb.best_trial\n",
    "res_results['lgb'] = trial_lgb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src- main_preds.py\n",
    "train_dat = get_data(1, split_pred=True)\n",
    "train_dat.head()\n",
    "\n",
    "# Example just predicting severity directly\n",
    "\n",
    "sat_500 = ['prop_lake_500', 'r_500', 'g_500', 'b_500']\n",
    "sat_1000 = ['prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']\n",
    "sat_2500 = ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500']\n",
    "sat_1025 = ['prop_lake_2500', 'r_2500', 'g_2500', 'b_2500', \n",
    "           'prop_lake_1000', 'r_1000', 'g_1000', 'b_1000']\n",
    "\n",
    "cat = mod.RegMod(ord_vars=['region','cluster'],\n",
    "                dat_vars=['date'],\n",
    "                ide_vars=['latitude','longitude','maxe','dife'],\n",
    "                y='severity',\n",
    "                mod = mod.CatBoostRegressor(iterations=380,depth=6,\n",
    "                   allow_writing_files=False,verbose=False)\n",
    "                )\n",
    "cat.fit(train_dat,weight=False,cat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = mod.RegMod(ord_vars=['region','cluster','imtype'],\n",
    "                dat_vars=['date'],\n",
    "                ide_vars=['latitude','longitude','elevation','dife'] + sat_1025,\n",
    "                y='severity',\n",
    "                mod = mod.LGBMRegressor(n_estimators=470,max_depth=8)\n",
    "                )\n",
    "lig.fit(train_dat,weight=False,cat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = mod.RegMod(ord_vars=['region','cluster'],\n",
    "                 dat_vars=['date'],\n",
    "                 y='severity',\n",
    "                 mod = mod.XGBRegressor(n_estimators=70, max_depth=2))\n",
    "xgb.fit(train_dat,weight=False,cat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = mod.EnsMod(mods={'xgb': xgb, 'cat': cat, 'lig': lig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now getting files for out of sample data\n",
    "test = feat.get_data(data_type='test')\n",
    "test['pred'] = rm.predict_int(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    5149\n",
      "1    1253\n",
      "3     108\n",
      "Name: severity, dtype: int64\n",
      " 0    2914\n",
      " 2    1836\n",
      " 1     968\n",
      "-1     561\n",
      " 3     227\n",
      "-2       4\n",
      "Name: dif_2023_02_16, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "form_dat = feat.sub_format(test)\n",
    "print(form_dat['severity'].value_counts())\n",
    "\n",
    "# function to check if similar to any past submissions\n",
    "mod.check_similar(form_dat)\n",
    "\n",
    "# Checking to see differences compared to best submission so far\n",
    "current = form_dat.copy()\n",
    "mod.check_day(current,day=\"sub_2023_02_16.csv\")\n",
    "current.groupby('region',as_index=False)['dif_2023_02_16'].value_counts()\n",
    "\n",
    "# Saving the data and model\n",
    "form_dat.to_csv(f'sub_BESTRESULTS_0414.csv',index=False)\n",
    "mod.save_model(rm,f'mod_BESTRESULTS_0414')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
